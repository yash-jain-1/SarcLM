{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d326d5de",
   "metadata": {},
   "source": [
    "# **Result**\n",
    "#### A BEFORE Vs AFTER comparison using a set of 20 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, json, csv, traceback\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43341835",
   "metadata": {},
   "source": [
    "### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "ANNOTATIONS_PATH = \"annotations.json\"\n",
    "OUTPUT_CSV = \"results_base.csv\"\n",
    "OUTPUT_JSONL = \"results_base.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device detection + CPU-fallback helper\n",
    "import torch, os\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {HAS_CUDA}\")\n",
    "\n",
    "def get_quant_config_or_none():\n",
    "    # Only attempt bitsandbytes when CUDA is present\n",
    "    if not HAS_CUDA:\n",
    "        print(\"[INFO] CUDA not available — skipping bitsandbytes quantization.\")\n",
    "        return None\n",
    "    try:\n",
    "        import bitsandbytes as bnb  # noqa: F401\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] bitsandbytes not available or failed ({e}). Falling back to fp16 (GPU) if available.\")\n",
    "        return None\n",
    "\n",
    "def load_base_model_and_processor_cpu_fallback(BASE_MODEL_NAME):\n",
    "    # This wraps the notebook's loader and chooses CPU-safe options when no GPU.\n",
    "    from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    qcfg = get_quant_config_or_none()\n",
    "\n",
    "    if HAS_CUDA and qcfg is not None:\n",
    "        print(\"[INFO] Using 4-bit quantization (CUDA).\")\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            quantization_config=qcfg,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    elif HAS_CUDA:\n",
    "        print(\"[INFO] CUDA available — loading model in fp16 with device_map='auto'.\")\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    else:\n",
    "        # CPU path: avoid fp16 and bitsandbytes; force CPU and low-ram-friendly flags\n",
    "        print(\"[INFO] Loading model on CPU (this will be slow and may OOM on low-RAM machines).\")\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            device_map={\"\": \"cpu\"},\n",
    "            low_cpu_mem_usage=True,   # helps reduce peak memory\n",
    "            torch_dtype=torch.float32,\n",
    "        )\n",
    "    # ensure pad token\n",
    "    tok = getattr(processor, \"tokenizer\", None)\n",
    "    if tok is not None and tok.pad_token_id is None:\n",
    "        if tok.eos_token_id is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    print(\"Model and processor loaded.\")\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5f72d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1477db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Base-only evaluation for LLaVA (Colab T4 friendly) ---\n",
    "# ---------- quant & loader (4-bit with fallback to fp16) ----------\n",
    "def get_quant_config_or_none():\n",
    "    try:\n",
    "        import bitsandbytes as _bnb  # noqa\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # T4: fp16 compute\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] 4-bit not available ({e}). Falling back to fp16.\")\n",
    "        return None\n",
    "\n",
    "def ensure_pad_token(model, processor):\n",
    "    tok = getattr(processor, \"tokenizer\", None)\n",
    "    if tok is not None and tok.pad_token_id is None:\n",
    "        if tok.eos_token_id is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "\n",
    "def load_base_model_and_processor():\n",
    "    print(\"Loading base LLaVA model + processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    qcfg = get_quant_config_or_none()\n",
    "\n",
    "    if qcfg:\n",
    "        print(\"[INFO] Using 4-bit quantization.\")\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            quantization_config=qcfg,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"[INFO] Using fp16.\")\n",
    "        model = AutoModelForImageTextToText.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    ensure_pad_token(model, processor)\n",
    "    print(\"Base model + processor loaded.\")\n",
    "    return model, processor\n",
    "\n",
    "# ---------- prompt & generation ----------\n",
    "def get_inference_prompt(utterance: str) -> str:\n",
    "    # LLaVA 1.5 conversational format\n",
    "    return f\"USER: <image>\\n{utterance}\\nASSISTANT:\"\n",
    "\n",
    "def safe_pad_id(model, processor):\n",
    "    tok = getattr(processor, \"tokenizer\", None)\n",
    "    if tok and tok.pad_token_id is not None:\n",
    "        return tok.pad_token_id\n",
    "    return getattr(model.config, \"eos_token_id\", None)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_response(model, processor, image_path: str, statement: str) -> str:\n",
    "    from PIL import Image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    prompt = get_inference_prompt(statement)\n",
    "\n",
    "    # ✅ Pass text/images by name; wrap as lists for batch dimension\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=[img],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=safe_pad_id(model, processor),\n",
    "    )\n",
    "\n",
    "    tok = processor.tokenizer\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    marker = \"ASSISTANT:\"\n",
    "    i = text.find(marker)\n",
    "    return text[i + len(marker):].strip() if i != -1 else text.strip()\n",
    "\n",
    "\n",
    "# ---------- data I/O ----------\n",
    "def load_annotations(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    raise ValueError(\"annotations.json must be a dict or a list of dicts.\")\n",
    "\n",
    "def maybe_fix_image_path(p: str) -> str:\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "    alt = p.replace(\"image_\", \"img_\") if \"image_\" in p else p.replace(\"img_\", \"image_\")\n",
    "    if alt != p and os.path.exists(alt):\n",
    "        return alt\n",
    "    joined = os.path.join(\"images\", os.path.basename(p))\n",
    "    if os.path.exists(joined):\n",
    "        return joined\n",
    "    return p  # may be missing; caller will handle\n",
    "\n",
    "# ---------- main eval ----------\n",
    "def evaluate_base(ann_path=ANNOTATIONS_PATH, out_csv=OUTPUT_CSV, out_jsonl=OUTPUT_JSONL):\n",
    "    model, processor = load_base_model_and_processor()\n",
    "    samples = load_annotations(ann_path)\n",
    "\n",
    "    fields = [\"id\",\"image\",\"statement\",\"label\",\"context\",\"explanation\",\"generated_text\",\"error\"]\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv, open(out_jsonl, \"w\", encoding=\"utf-8\") as fjl:\n",
    "        writer = csv.DictWriter(fcsv, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for ex in samples:\n",
    "            row = {\n",
    "                \"id\": ex.get(\"id\",\"\"),\n",
    "                \"image\": ex.get(\"image\",\"\"),\n",
    "                \"statement\": ex.get(\"statement\",\"\"),\n",
    "                \"label\": ex.get(\"label\",\"\"),\n",
    "                \"context\": ex.get(\"context\",\"\"),\n",
    "                \"explanation\": ex.get(\"explanation\",\"\"),\n",
    "                \"generated_text\": \"\",\n",
    "                \"error\": \"\",\n",
    "            }\n",
    "            img_path = maybe_fix_image_path(row[\"image\"])\n",
    "            try:\n",
    "                if not os.path.exists(img_path):\n",
    "                    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "                row[\"generated_text\"] = generate_response(model, processor, img_path, row[\"statement\"])\n",
    "            except Exception as e:\n",
    "                row[\"error\"] = f\"{type(e).__name__}: {e}\"\n",
    "                traceback.print_exc()\n",
    "\n",
    "            writer.writerow(row)\n",
    "            fjl.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[DONE] Wrote:\\n - {out_csv}\\n - {out_jsonl}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e945af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Clean up any wrong wheels\n",
    "!pip uninstall -y bitsandbytes\n",
    "\n",
    "# 2) Install CUDA 12-compatible wheel\n",
    "!pip install -U bitsandbytes --extra-index-url https://jllllll.github.io/bitsandbytes-wheels/cu12\n",
    "\n",
    "# 3) Core deps\n",
    "!pip install -U \"transformers>=4.44\" \"accelerate>=0.33\" \"peft>=0.12\"\n",
    "\n",
    "# (Optional) sanity check GPU\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
