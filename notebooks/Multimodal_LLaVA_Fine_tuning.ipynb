{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRBxnKOMADkp",
        "outputId": "b55db14d-75a3-4890-8799-3accd9972214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -q bitsandbytes>=0.39.0\n",
        "!pip install -q -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149,
          "referenced_widgets": [
            "aa99334d4adc46a1bdb55b502d8dc2e4",
            "61765227771341cdbc4c804321d0cb0e",
            "f4e3d9d2c05040479487f68e98d28640",
            "20ff98cd10484639bf91923d915bfbca",
            "fad32e1c13a043e2a01b11c75d9a8a2a",
            "52d0fe7293ff4f2db5c62a653617e7cd",
            "30eee9a856b7484cafccd7d139c3494c",
            "aadd7838ca7e45c99b228ab692af7f52",
            "4812f74882ff454b9d21675f3e670e96",
            "fdcbef3859fe466daf353213ccca52b3",
            "e0704c4a77a24d1d86e72872c3e85674"
          ]
        },
        "id": "CcW9gCad7ISc",
        "outputId": "35458076-2697-4e4d-adcf-c0fa95f45f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and processor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa99334d4adc46a1bdb55b502d8dc2e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    # AutoModelForCausalLM, # This was causing the error for LLaVA\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForVision2Seq # Correct class for multimodal LLaVA models\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "\n",
        "# Model and processor names\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "# Input data paths\n",
        "ANNOTATIONS_PATH = \"annotations.json\"\n",
        "IMAGE_DIR = \"images/\"\n",
        "\n",
        "# Output directory for the fine-tuned model adapters\n",
        "OUTPUT_DIR = \"./llava-finetuned-adapters-multimodal\"\n",
        "\n",
        "# --- 2. Load Model and Processor with Quantization ---\n",
        "\n",
        "def load_model_and_processor():\n",
        "    \"\"\"Loads the LLaVA model and processor with 4-bit quantization.\"\"\"\n",
        "    print(\"Loading model and processor...\")\n",
        "\n",
        "    # Configure quantization to reduce memory usage\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Load the base model using AutoModelForVision2Seq\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        BASE_MODEL_NAME,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # Load the LLaVA processor (handles both image and text)\n",
        "    processor = AutoProcessor.from_pretrained(BASE_MODEL_NAME)\n",
        "    # Set padding token\n",
        "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "\n",
        "    print(\"Model and processor loaded successfully.\")\n",
        "    return model, processor\n",
        "\n",
        "# --- 3. Configure PEFT/LoRA ---\n",
        "\n",
        "def get_peft_config(model):\n",
        "    \"\"\"\n",
        "    Configures the LoRA parameters, crucially targeting the\n",
        "    multimodal projector and other key attention layers.\n",
        "    \"\"\"\n",
        "    print(\"Configuring LoRA...\")\n",
        "\n",
        "    # Find all linear layers for LoRA target modules\n",
        "    # This is a robust way to target all relevant layers\n",
        "    target_modules = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear) and \"lm_head\" not in name:\n",
        "            # We add all linear layers, which will include:\n",
        "            # - q_proj, k_proj, v_proj, o_proj (Attention)\n",
        "            # - gate_proj, up_proj, down_proj (Feed-forward)\n",
        "            # - mm_projector (The crucial vision-language connector)\n",
        "            target_modules.append(name.split('.')[-1])\n",
        "\n",
        "    # Remove duplicates\n",
        "    target_modules = sorted(list(set(target_modules)))\n",
        "\n",
        "    # Ensure key modules are present (optional check)\n",
        "    print(f\"LoRA Target Modules: {target_modules}\")\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules\n",
        "    )\n",
        "    print(\"LoRA configured.\")\n",
        "    return peft_config\n",
        "\n",
        "# --- 4. Dataset Preprocessing ---\n",
        "\n",
        "class MultimodalSarcasmDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset to load images and text from\n",
        "    our annotations.json file.\n",
        "    \"\"\"\n",
        "    def __init__(self, annotations_path, image_dir, processor):\n",
        "        print(f\"Loading annotations from {annotations_path}\")\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            self.annotations = json.load(f)\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations[idx]\n",
        "\n",
        "        # 1. Format the text prompt\n",
        "        statement = item['statement']\n",
        "\n",
        "        # This is the target output the model must learn\n",
        "        response = f\"Label: {item['label']}\\nExplanation: {item['explanation']}\"\n",
        "\n",
        "        # This is the LLaVA 1.5 prompt format\n",
        "        prompt = f\"USER: <image>\\n{statement}\\nASSISTANT:{response}{self.processor.tokenizer.eos_token}\"\n",
        "\n",
        "        # 2. Load the image\n",
        "        image_path = os.path.join(self.image_dir, item['image'])\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found at {image_path}. Using a blank image.\")\n",
        "            image = Image.new('RGB', (512, 512), (255, 255, 255)) # Blank white image\n",
        "\n",
        "        # 3. Process image and text\n",
        "        # The processor will handle image preprocessing and text tokenization\n",
        "        # We set `padding=True`, `truncation=True` in the data collator\n",
        "        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "        # The processor output is nested, e.g., inputs['input_ids'][0]\n",
        "        # We want to return a flat dictionary\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # We need to create the 'labels' for supervised fine-tuning\n",
        "        # The labels are the same as the input_ids\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "\n",
        "        # Mask out the \"USER\" part of the prompt from the labels\n",
        "        # The model should only learn to predict the \"ASSISTANT\" part\n",
        "\n",
        "        # Let's find the start of the assistant's response\n",
        "        # Tokenize the prompt *without* the response\n",
        "        prompt_user_part = f\"USER: <image>\\n{statement}\\nASSISTANT:\"\n",
        "        user_inputs = self.processor(text=prompt_user_part, images=image, return_tensors=\"pt\")\n",
        "        user_input_len = len(user_inputs['input_ids'][0])\n",
        "\n",
        "        # Mask all tokens up to the start of the assistant's response\n",
        "        # (minus 1, as the first token of the response is at that index)\n",
        "        labels = inputs['labels']\n",
        "        labels[:user_input_len - 1] = -100 # -100 is the ignore_index for cross-entropy loss\n",
        "\n",
        "        # Replace pad tokens in labels with -100\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        inputs['labels'] = labels\n",
        "\n",
        "        return inputs\n",
        "\n",
        "def get_data_collator(processor):\n",
        "    \"\"\"\n",
        "    Return a data collator that handles padding for our multimodal inputs.\n",
        "    \"\"\"\n",
        "    def data_collator(features):\n",
        "        # Pad text inputs\n",
        "        input_ids = [f['input_ids'] for f in features]\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        attention_mask = (input_ids != processor.tokenizer.pad_token_id)\n",
        "\n",
        "        # Pad labels\n",
        "        labels = [f['labels'] for f in features]\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(\n",
        "            labels, batch_first=True, padding_value=-100\n",
        "        )\n",
        "\n",
        "        # Stack pixel values\n",
        "        pixel_values = torch.stack([f['pixel_values'] for f in features])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': labels,\n",
        "        }\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "# --- 5. Main Training Function ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the fine-tuning process.\"\"\"\n",
        "\n",
        "    # Step 1: Load model and processor\n",
        "    model, processor = load_model_and_processor()\n",
        "\n",
        "    # Step 2: Configure LoRA\n",
        "    peft_config = get_peft_config(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Step 3: Load and prepare the dataset\n",
        "    dataset = MultimodalSarcasmDataset(\n",
        "        annotations_path=ANNOTATIONS_PATH,\n",
        "        image_dir=IMAGE_DIR,\n",
        "        processor=processor\n",
        "    )\n",
        "\n",
        "    # Step 4: Get data collator\n",
        "    data_collator = get_data_collator(processor)\n",
        "\n",
        "    # Step 5: Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=3,              # More epochs are often needed for this\n",
        "        per_device_train_batch_size=2,   # LLaVA is memory-hungry! Start small.\n",
        "        gradient_accumulation_steps=8,   # Effective batch size = 16\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        learning_rate=1e-4,              # A slightly lower LR can be stable\n",
        "        weight_decay=0.001,\n",
        "        fp16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type=\"constant\",\n",
        "        remove_unused_columns=False, # We need 'pixel_values'\n",
        "        report_to=\"none\", # Disable wandb/tensorboard logging for simplicity\n",
        "    )\n",
        "\n",
        "    # Step 6: Initialize the Trainer\n",
        "    print(\"Initializing trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    # Step 7: Start training\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Step 8: Save the fine-tuned model adapters\n",
        "    print(f\"Saving fine-tuned model adapters to {OUTPUT_DIR}...\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    processor.save_pretrained(OUTPUT_DIR) # Save the processor too\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: image file not found: images/ironic_scene.jpg\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_KEY\")  # set this in your environment, do NOT hardcode\n",
        "HF_MODEL = \"Salesforce/blip2-flan-t5-base\"  # model slug on HF Hub\n",
        "\n",
        "def get_multimodal_inference_response_hf_router(image_path: str, text_prompt: str, timeout: int = 60) -> str:\n",
        "    \"\"\"\n",
        "    Uses the HF Router inference endpoint:\n",
        "    POST https://router.huggingface.co/hf-inference/{model}\n",
        "    multipart/form-data: file + inputs (prompt)\n",
        "    \"\"\"\n",
        "    if not HF_TOKEN:\n",
        "        return \"Error: HF_TOKEN environment variable not set.\"\n",
        "\n",
        "    # router endpoint (note 'hf-inference' path)\n",
        "    url = f\"https://router.huggingface.co/hf-inference/{HF_MODEL}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as f:\n",
        "            img_bytes = f.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: image file not found: {image_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading image file: {e}\"\n",
        "\n",
        "    # multipart POST: file + inputs (the prompt)\n",
        "    files = {\n",
        "        \"file\": (\"image.jpg\", img_bytes, \"image/jpeg\"),\n",
        "        \"inputs\": (None, text_prompt),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.post(url, headers=headers, files=files, timeout=timeout)\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Network error while calling HF Router: {e}\"\n",
        "\n",
        "    # handle non-200\n",
        "    if resp.status_code != 200:\n",
        "        return f\"Hugging Face Router error {resp.status_code}: {resp.text}\"\n",
        "\n",
        "    try:\n",
        "        data = resp.json()\n",
        "    except ValueError:\n",
        "        # sometimes the response is plain text\n",
        "        return resp.text\n",
        "\n",
        "    # Common shapes:\n",
        "    # 1) [{\"generated_text\": \"...\"}]\n",
        "    # 2) {\"generated_text\": \"...\"}\n",
        "    # 3) other JSON shapes or ID-structured responses\n",
        "    if isinstance(data, list) and len(data) and isinstance(data[0], dict) and \"generated_text\" in data[0]:\n",
        "        return data[0][\"generated_text\"]\n",
        "    if isinstance(data, dict) and \"generated_text\" in data:\n",
        "        return data[\"generated_text\"]\n",
        "\n",
        "    # fallback: stringify useful keys if available\n",
        "    return str(data)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    p = \"images/ironic_scene.jpg\"\n",
        "    prompt = \"Describe the scene and indicate whether the caption is sarcastic.\"\n",
        "    print(get_multimodal_inference_response_hf_router(p, prompt))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20ff98cd10484639bf91923d915bfbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdcbef3859fe466daf353213ccca52b3",
            "placeholder": "​",
            "style": "IPY_MODEL_e0704c4a77a24d1d86e72872c3e85674",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "30eee9a856b7484cafccd7d139c3494c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4812f74882ff454b9d21675f3e670e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52d0fe7293ff4f2db5c62a653617e7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61765227771341cdbc4c804321d0cb0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d0fe7293ff4f2db5c62a653617e7cd",
            "placeholder": "​",
            "style": "IPY_MODEL_30eee9a856b7484cafccd7d139c3494c",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "aa99334d4adc46a1bdb55b502d8dc2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61765227771341cdbc4c804321d0cb0e",
              "IPY_MODEL_f4e3d9d2c05040479487f68e98d28640",
              "IPY_MODEL_20ff98cd10484639bf91923d915bfbca"
            ],
            "layout": "IPY_MODEL_fad32e1c13a043e2a01b11c75d9a8a2a"
          }
        },
        "aadd7838ca7e45c99b228ab692af7f52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0704c4a77a24d1d86e72872c3e85674": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4e3d9d2c05040479487f68e98d28640": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aadd7838ca7e45c99b228ab692af7f52",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4812f74882ff454b9d21675f3e670e96",
            "value": 0
          }
        },
        "fad32e1c13a043e2a01b11c75d9a8a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdcbef3859fe466daf353213ccca52b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
