{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ocl6OYkmNUPQ",
    "outputId": "2f13de0e-3d2f-4248-9a55-306dc654c798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run only if not already installed)\n",
    "!pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DR9TfOdDxED6",
    "outputId": "9941ac84-c2b5-4279-e394-e41e77097c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5bea8a9d3c6d442e80e9cb7da56ed49c",
      "b50ffbafbc9f4431b8a8319398481be5",
      "00c2c360c00b433198d7794766b8fc94",
      "5722894bcb4341af81a742885df17624",
      "3a1ab45685144e358379b31dba9cfd06",
      "7fddae1dad014071b9a4b8a8f314938a",
      "a9c691cb98da4852ad1006b11d8bcc84",
      "bc63f284ddca4935870b39c8e35f9e1d",
      "c5ccb978fc3342f2a146f928c3df81da",
      "6a92e65d695346ffa1a074b54905413f",
      "06f15803e64c40eda5f0d66401954020",
      "b21e1a5f3e284c329b0cf3aebfc759b1",
      "f527fe0bfe33431aaf85657284bdd55a",
      "f911848931644c318936f317489a5f3f",
      "191ee0d353d246d6b7a8dd25e1c58a94",
      "030b067bcdc844a3b9c879d5abb74126",
      "68558251812240b39af7eed8d0362a39",
      "b4723b9dd89448e487cafdbeda5b243e",
      "1022261b5d094e8080b1286c18726397",
      "14667dc4dd3543868ef05786db88176c",
      "497949a5549b47c9b4beade60a2baef5",
      "b2428a724ae540ab8c9de1f3407718cf",
      "122794ba89c0404486853ead0ffc8415",
      "5fd3706c7a484221bf5623d86ce0597f",
      "f2baf8308a16400098c898ed8fb70786",
      "b5cc6a4766984da8905c083c448eb5bc",
      "a69cfa8f014240d483f9378f1fb9e68a",
      "5f733d182671436d922614cc3c477766",
      "5847bed556e646d287567fa5fcda35be",
      "53ec97ad40324e6db93b8f66079fe550",
      "ca8284c21fe44236aabd4cb0e2ac1104",
      "474fac95e87047a1b985dc7eeb42df6c",
      "73b4ea90c09f4907b3014bacc450ea30"
     ]
    },
    "id": "LHDLdz3fDART",
    "outputId": "1de44e9b-c3d5-4c79-fbee-333fae5ef9ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu126\n",
      "transformers: 4.56.2\n",
      "bitsandbytes: 0.47.0\n",
      "Loading config with trust_remote_code=True (this will fetch repo's custom code).\n",
      "Config class: LlavaConfig\n",
      "LlavaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"dtype\": \"float16\",\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_seq_length\": 576,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"multimodal_projector_bias\": true,\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"dtype\": \"float16\",\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 4096,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"vision_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "Detected LlavaConfig (multimodal/custom). Attempting to load repo model class directly.\n",
      "Found class LlavaForConditionalGeneration in module transformers.models.llava.modeling_llava, loading model with it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bea8a9d3c6d442e80e9cb7da56ed49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21e1a5f3e284c329b0cf3aebfc759b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122794ba89c0404486853ead0ffc8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Sample module names (first 50):\n",
      "0 \n",
      "1 model\n",
      "2 model.vision_tower\n",
      "3 model.vision_tower.vision_model\n",
      "4 model.vision_tower.vision_model.embeddings\n",
      "5 model.vision_tower.vision_model.embeddings.patch_embedding\n",
      "6 model.vision_tower.vision_model.embeddings.position_embedding\n",
      "7 model.vision_tower.vision_model.pre_layrnorm\n",
      "8 model.vision_tower.vision_model.encoder\n",
      "9 model.vision_tower.vision_model.encoder.layers\n",
      "10 model.vision_tower.vision_model.encoder.layers.0\n",
      "11 model.vision_tower.vision_model.encoder.layers.0.self_attn\n",
      "12 model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj\n",
      "13 model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj\n",
      "14 model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj\n",
      "15 model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj\n",
      "16 model.vision_tower.vision_model.encoder.layers.0.layer_norm1\n",
      "17 model.vision_tower.vision_model.encoder.layers.0.mlp\n",
      "18 model.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn\n",
      "19 model.vision_tower.vision_model.encoder.layers.0.mlp.fc1\n",
      "20 model.vision_tower.vision_model.encoder.layers.0.mlp.fc2\n",
      "21 model.vision_tower.vision_model.encoder.layers.0.layer_norm2\n",
      "22 model.vision_tower.vision_model.encoder.layers.1\n",
      "23 model.vision_tower.vision_model.encoder.layers.1.self_attn\n",
      "24 model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj\n",
      "25 model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj\n",
      "26 model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj\n",
      "27 model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj\n",
      "28 model.vision_tower.vision_model.encoder.layers.1.layer_norm1\n",
      "29 model.vision_tower.vision_model.encoder.layers.1.mlp\n",
      "30 model.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn\n",
      "31 model.vision_tower.vision_model.encoder.layers.1.mlp.fc1\n",
      "32 model.vision_tower.vision_model.encoder.layers.1.mlp.fc2\n",
      "33 model.vision_tower.vision_model.encoder.layers.1.layer_norm2\n",
      "34 model.vision_tower.vision_model.encoder.layers.2\n",
      "35 model.vision_tower.vision_model.encoder.layers.2.self_attn\n",
      "36 model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj\n",
      "37 model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj\n",
      "38 model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj\n",
      "39 model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj\n",
      "40 model.vision_tower.vision_model.encoder.layers.2.layer_norm1\n",
      "41 model.vision_tower.vision_model.encoder.layers.2.mlp\n",
      "42 model.vision_tower.vision_model.encoder.layers.2.mlp.activation_fn\n",
      "43 model.vision_tower.vision_model.encoder.layers.2.mlp.fc1\n",
      "44 model.vision_tower.vision_model.encoder.layers.2.mlp.fc2\n",
      "45 model.vision_tower.vision_model.encoder.layers.2.layer_norm2\n",
      "46 model.vision_tower.vision_model.encoder.layers.3\n",
      "47 model.vision_tower.vision_model.encoder.layers.3.self_attn\n",
      "48 model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj\n",
      "49 model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# robust_llava_loader.py\n",
    "# Loads a LLaVA-style model even when AutoModelForCausalLM doesn't recognize LlavaConfig.\n",
    "# Requires: transformers, torch, datasets, peft, trl, etc.\n",
    "# Make sure to run: pip install -U \"transformers>=4.31.0\" bitsandbytes peft trl datasets safetensors\n",
    "# if you want 4-bit quantization support (and bitsandbytes installed).\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import traceback\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # if bitsandbytes present; import may fail if not installed\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CONFIG\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "USE_4BIT = False  # set True only if bitsandbytes is installed & you want 4-bit quant\n",
    "DEVICE_MAP = \"auto\"\n",
    "LOW_CPU_MEM = True\n",
    "\n",
    "def print_versions():\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        print(\"bitsandbytes:\", bnb.__version__)\n",
    "    except Exception:\n",
    "        print(\"bitsandbytes: NOT INSTALLED\")\n",
    "\n",
    "def try_load_llava_class_and_model(model_name, quant_config=None, device_map=\"auto\", low_cpu_mem=True):\n",
    "    candidate_module_paths = [\n",
    "        \"transformers.models.llava.modeling_llava\",\n",
    "        \"transformers.models.llava.modeling_llava_for_causal_lm\",\n",
    "        \"llava.modeling_llava\",\n",
    "        \"modeling_llava\",\n",
    "    ]\n",
    "    candidate_class_names = [\n",
    "        \"LlavaForCausalLM\",\n",
    "        \"LlavaModelForCausalLM\",\n",
    "        \"LlavaForConditionalGeneration\",\n",
    "        \"LlavaModel\",\n",
    "        \"LlavaForVision2Seq\",\n",
    "    ]\n",
    "\n",
    "    last_exc = None\n",
    "    for mod_path in candidate_module_paths:\n",
    "        try:\n",
    "            module = importlib.import_module(mod_path)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            continue\n",
    "\n",
    "        for cls_name in candidate_class_names:\n",
    "            ModelClass = getattr(module, cls_name, None)\n",
    "            if ModelClass is None:\n",
    "                continue\n",
    "\n",
    "            # Try strategy sequence:\n",
    "            # 1) If quant_config provided -> try direct (fast path)\n",
    "            # 2) If ValueError complaining about dispatch -> retry with llm_int8_enable_fp32_cpu_offload + device_map=\"auto\"\n",
    "            # 3) If still failing -> fallback to no-quant (float16)\n",
    "            try:\n",
    "                print(f\"Trying to load {cls_name} with device_map={device_map} (quant_config={'yes' if quant_config else 'no'})...\")\n",
    "                return _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs={})\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                tb = traceback.format_exc()\n",
    "                print(f\"Initial attempt with {cls_name} failed: {e}\\n{tb}\")\n",
    "\n",
    "                # If message suggests offload, try offload route (only if quant_config not None)\n",
    "                msg = str(e).lower()\n",
    "                if quant_config is not None and (\"offload\" in msg or \"dispatched on the cpu\" in msg or \"some modules are dispatched\" in msg):\n",
    "                    try:\n",
    "                        print(\"Retrying with llm_int8_enable_fp32_cpu_offload=True and device_map='auto'...\")\n",
    "                        return ModelClass.from_pretrained(\n",
    "                            model_name,\n",
    "                            quantization_config=quant_config,\n",
    "                            device_map=\"auto\",\n",
    "                            trust_remote_code=True,\n",
    "                            low_cpu_mem_usage=low_cpu_mem,\n",
    "                            llm_int8_enable_fp32_cpu_offload=True,\n",
    "                        )\n",
    "                    except Exception as e2:\n",
    "                        last_exc = e2\n",
    "                        tb2 = traceback.format_exc()\n",
    "                        print(f\"Retry with offload failed: {e2}\\n{tb2}\")\n",
    "\n",
    "                # Final fallback: try without quantization (float16)\n",
    "                try:\n",
    "                    print(\"Retrying without quantization (float16) as a fallback...\")\n",
    "                    return ModelClass.from_pretrained(\n",
    "                        model_name,\n",
    "                        device_map=device_map,\n",
    "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                        trust_remote_code=True,\n",
    "                        low_cpu_mem_usage=low_cpu_mem,\n",
    "                    )\n",
    "                except Exception as e3:\n",
    "                    last_exc = e3\n",
    "                    tb3 = traceback.format_exc()\n",
    "                    print(f\"Fallback without quantization also failed: {e3}\\n{tb3}\")\n",
    "                    continue\n",
    "\n",
    "    raise RuntimeError(\"Tried candidate Llava classes but all failed. Last exception:\\n\" + (str(last_exc) if last_exc is not None else \"None\"))\n",
    "\n",
    "def _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs):\n",
    "    \"\"\"Helper to call from_pretrained with given kwargs and bubble exceptions.\"\"\"\n",
    "    try:\n",
    "        if quant_config is not None:\n",
    "            return ModelClass.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "                **extra_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            return ModelClass.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "                **extra_kwargs,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def load_model_and_tokenizer(model_name, use_4bit=True, device_map=\"auto\", low_cpu_mem=True):\n",
    "    # Load config + tokenizer\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    quant_config = None\n",
    "    if use_4bit:\n",
    "        try:\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Could not create BitsAndBytesConfig:\", e)\n",
    "            quant_config = None\n",
    "\n",
    "    # If LlavaConfig detected: try repo model classes (with fallback paths)\n",
    "    cfg_name = config.__class__.__name__.lower()\n",
    "    if \"llava\" in cfg_name:\n",
    "        model = try_load_llava_class_and_model(model_name, quant_config=quant_config, device_map=device_map, low_cpu_mem=low_cpu_mem)\n",
    "    else:\n",
    "        # generic fallback to AutoModelForCausalLM\n",
    "        if quant_config is not None:\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=quant_config,\n",
    "                    device_map=device_map,\n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=low_cpu_mem,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,  # safe to include\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"AutoModelForCausalLM with quant failed, retrying without quant:\", e)\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=device_map,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=low_cpu_mem,\n",
    "                )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "\n",
    "# Now, when you run main(), it will automatically retry with llm_int8_enable_fp32_cpu_offload if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r86yQYcGDDQp",
    "outputId": "2e2bd0c4-2871-468a-ab13-37f901f75c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Running in Colab. You may use Colab resources.\")\n",
    "    # Optionally: get API key from user input or environment\n",
    "    # api_key = input(\"Enter your API key: \")\n",
    "else:\n",
    "    print(\"Not running in Colab. Using local environment.\")\n",
    "    # Optionally: get API key from environment variable\n",
    "    # api_key = os.getenv(\"YOUR_API_KEY_ENV_VAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "B524hMN5ZD7H",
    "outputId": "8e4d9057-d243-4aff-969f-e52652ee0159"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_model_and_tokenizer() missing 1 required positional argument: 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-821453568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded. Sample module names (first 50):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_model_and_tokenizer() missing 1 required positional argument: 'model_name'"
     ]
    }
   ],
   "source": [
    "# (robust_llava_loader.py code is already present above, so this cell is now redundant and can be removed)\n",
    "# (Removed duplicate robust_llava_loader.py code for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "evPygyPuUVXU",
    "outputId": "7108007a-5c04-46fc-b357-0b35ae1b0a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model + tokenizer (robust loader)...\n",
      "torch: 2.8.0+cu126\n",
      "transformers: 4.56.2\n",
      "bitsandbytes: 0.47.0\n",
      "Loading config with trust_remote_code=True (this will fetch repo's custom code).\n",
      "Config class: LlavaConfig\n",
      "LlavaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"dtype\": \"float16\",\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_seq_length\": 576,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"multimodal_projector_bias\": true,\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"dtype\": \"float16\",\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 4096,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"vision_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 336,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "Detected LlavaConfig (multimodal/custom). Attempting to load repo model class directly.\n",
      "Found class LlavaForConditionalGeneration in module transformers.models.llava.modeling_llava, loading model with it...\n",
      "Attempt to load with LlavaForConditionalGeneration failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Found class LlavaModel in module transformers.models.llava.modeling_llava, loading model with it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/robust_llava_loader.py\", line 72, in try_load_llava_class_and_model\n",
      "    # If message suggests offload, try offload route (only if quant_config not None)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5160, in from_pretrained\n",
      "    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 1473, in _get_device_map\n",
      "    hf_quantizer.validate_environment(device_map=device_map)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 117, in validate_environment\n",
      "    raise ValueError(\n",
      "ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to load with LlavaModel failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/robust_llava_loader.py\", line 72, in try_load_llava_class_and_model\n",
      "    # If message suggests offload, try offload route (only if quant_config not None)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5160, in from_pretrained\n",
      "    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 1473, in _get_device_map\n",
      "    hf_quantizer.validate_environment(device_map=device_map)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 117, in validate_environment\n",
      "    raise ValueError(\n",
      "ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not find a LLaVA model class in the remote repo modules. Tried candidate modules and classes but failed. Last exception:\nNo module named 'modeling_llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1433311502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1433311502.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# 1) Load model + tokenizer (robust loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model + tokenizer (robust loader)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_4bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model and tokenizer. Model dtype hint:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/robust_llava_loader.py\u001b[0m in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_name, use_4bit)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Load config + tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/robust_llava_loader.py\u001b[0m in \u001b[0;36mtry_load_llava_class_and_model\u001b[0;34m(model_name, quant_config, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m                         \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                         \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                         \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                         \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not find a LLaVA model class in the remote repo modules. Tried candidate modules and classes but failed. Last exception:\nNo module named 'modeling_llava'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune_after_script_b.py\n",
    "\n",
    "Uses robust_llava_loader.load_model_and_tokenizer() (script B output) to load the model,\n",
    "auto-detects good LoRA target_modules, and runs PEFT (LoRA) fine-tuning with trl.SFTTrainer.\n",
    "\n",
    "Usage: python finetune_after_script_b.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# try to import helper for preparing k-bit training (peft versions vary)\n",
    "try:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "except Exception:\n",
    "    try:\n",
    "        from peft.utils import prepare_model_for_kbit_training\n",
    "    except Exception:\n",
    "        prepare_model_for_kbit_training = None\n",
    "\n",
    "# robust loader from script B\n",
    "from robust_llava_loader import load_model_and_tokenizer\n",
    "\n",
    "# --------------------- USER CONFIG ---------------------\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "DATASET_PATH = \"meld_with_rationales.jsonl\"   # jsonl containing utterance, sentiment, rationale\n",
    "OUTPUT_DIR = \"./llava-peft-adapters-auto\"\n",
    "USE_4BIT_IF_AVAILABLE = True\n",
    "MAX_SEQ_LENGTH = 512\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "SAVE_STEPS = 200\n",
    "LOGGING_STEPS = 20\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# helper: create the training prompt\n",
    "def build_prompt(example):\n",
    "    return (\n",
    "        \"You are a sentiment analysis expert. Analyze the following utterance and provide \"\n",
    "        \"the sentiment along with a step-by-step rationale for your decision.\\n\\n\"\n",
    "        \"### Utterance:\\n\"\n",
    "        f\"{example.get('utterance','')}\\n\\n\"\n",
    "        \"### Analysis:\\n\"\n",
    "        f\"Sentiment: {example.get('sentiment','')}\\n\"\n",
    "        f\"Rationale: {example.get('rationale','')}\"\n",
    "    )\n",
    "\n",
    "# helper: scan model.named_modules() and choose candidate target module name substrings\n",
    "def auto_detect_target_module_names(model, prefer_text=True):\n",
    "    \"\"\"\n",
    "    Returns a list of module-name substrings to use in LoraConfig.target_modules.\n",
    "    Strategy:\n",
    "      - Collect names of submodules that look like projections (q_proj, k_proj, v_proj, out_proj, o_proj)\n",
    "      - Prefer modules under 'model' that contain tokens like 'self_attn', 'attn', 'q_proj' etc.\n",
    "      - If prefer_text=True, try to exclude modules under vision tower (module name containing 'vision' or 'vision_tower')\n",
    "    \"\"\"\n",
    "    proj_patterns = set()\n",
    "    name_list = [n for n, _ in model.named_modules()]\n",
    "\n",
    "    for n in name_list:\n",
    "        # skip top-level empty name\n",
    "        if not n:\n",
    "            continue\n",
    "        # skip vision modules if preferring text modules\n",
    "        if prefer_text and (\"vision\" in n or \"vision_tower\" in n or \"vision_model\" in n):\n",
    "            continue\n",
    "        # find typical projection/fc names in module path\n",
    "        if re.search(r\"(q_proj|k_proj|v_proj|o_proj|out_proj|gate_proj|up_proj|down_proj|fc1|fc2|mlp)\", n):\n",
    "            # extract final token (last part after '.')\n",
    "            final = n.split(\".\")[-1]\n",
    "            proj_patterns.add(final)\n",
    "    # fallback if empty\n",
    "    if not proj_patterns:\n",
    "        # default common names\n",
    "        proj_patterns = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
    "    # keep consistent ordering and return as list\n",
    "    return sorted(list(proj_patterns))\n",
    "\n",
    "def format_and_map(example, tokenizer):\n",
    "    text = build_prompt(example)\n",
    "    eos = tokenizer.eos_token or \"\"\n",
    "    return {\"text\": text + eos}\n",
    "\n",
    "def main():\n",
    "    # decide 4-bit usage\n",
    "    use_4bit = False\n",
    "    if USE_4BIT_IF_AVAILABLE:\n",
    "        try:\n",
    "            import bitsandbytes  # noqa: F401\n",
    "            use_4bit = True\n",
    "        except Exception:\n",
    "            print(\"bitsandbytes not installed/found — running without 4-bit.\")\n",
    "\n",
    "    # 1) Load model + tokenizer (robust loader)\n",
    "    print(\"Loading model + tokenizer (robust loader)...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name=BASE_MODEL_NAME, use_4bit=use_4bit)\n",
    "    print(\"Loaded model and tokenizer. Model dtype hint:\", getattr(model, \"dtype\", None))\n",
    "    model.config.use_cache = False\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Auto-detect target_modules for LoRA (based on model module names)\n",
    "    print(\"Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\")\n",
    "    detected = auto_detect_target_module_names(model, prefer_text=True)\n",
    "    print(\"Detected target-module name tokens (candidates):\", detected)\n",
    "\n",
    "    # We'll use these tokens as LoraConfig.target_modules (PEFT expects substrings)\n",
    "    target_modules = detected\n",
    "\n",
    "    # 3) Prepare model for k-bit training (if using 4-bit & helper present)\n",
    "    if use_4bit:\n",
    "        if prepare_model_for_kbit_training is not None:\n",
    "            print(\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\")\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        else:\n",
    "            print(\"prepare_model_for_kbit_training not available in this peft version — continuing.\")\n",
    "\n",
    "    # 4) Create LoraConfig and wrap model\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    print(\"Applying LoRA with LoraConfig:\", lora_cfg)\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    print(\"PEFT/LoRA applied. Peft model keys:\", list(model.named_parameters())[:5])\n",
    "\n",
    "    # 5) Load and format dataset\n",
    "    print(\"Loading dataset from\", DATASET_PATH)\n",
    "    ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "    print(\"Dataset size:\", len(ds))\n",
    "    # map to `text` field expected by SFTTrainer\n",
    "    ds = ds.map(lambda ex: format_and_map(ex, tokenizer), remove_columns=ds.column_names)\n",
    "\n",
    "    # 6) TrainingArguments + trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=use_4bit or (torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 12 * 1024 ** 2),\n",
    "        save_steps=SAVE_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        save_total_limit=3,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # import here to avoid top-level dependency until needed\n",
    "    from trl import SFTTrainer\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ds,\n",
    "        peft_config=lora_cfg,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    # 7) Dry-run: single step to validate forward/backward\n",
    "    print(\"Running a 1-step dry-run to validate training loop...\")\n",
    "    try:\n",
    "        trainer.train(max_steps=1)\n",
    "        print(\"Dry-run succeeded.\")\n",
    "    except Exception as e:\n",
    "        print(\"Dry-run failed — inspect traceback. Error:\", e)\n",
    "        raise\n",
    "\n",
    "    # 8) Full training\n",
    "    print(\"Starting full training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # 9) Save PEFT adapters\n",
    "    print(\"Saving adapters to:\", OUTPUT_DIR)\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(\"Saved. You can load later with PeftModel.from_pretrained(base_model, OUTPUT_DIR)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAFcBo8uVALa"
   },
   "outputs": [],
   "source": [
    "# (This cell was a duplicate of robust_llava_loader.py logic and is now removed for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkKd6kxXXHG6"
   },
   "outputs": [],
   "source": [
    "# (Empty cell placeholder removed for clarity)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c2c360c00b433198d7794766b8fc94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc63f284ddca4935870b39c8e35f9e1d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5ccb978fc3342f2a146f928c3df81da",
      "value": 3
     }
    },
    "030b067bcdc844a3b9c879d5abb74126": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06f15803e64c40eda5f0d66401954020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1022261b5d094e8080b1286c18726397": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "122794ba89c0404486853ead0ffc8415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5fd3706c7a484221bf5623d86ce0597f",
       "IPY_MODEL_f2baf8308a16400098c898ed8fb70786",
       "IPY_MODEL_b5cc6a4766984da8905c083c448eb5bc"
      ],
      "layout": "IPY_MODEL_a69cfa8f014240d483f9378f1fb9e68a"
     }
    },
    "14667dc4dd3543868ef05786db88176c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "191ee0d353d246d6b7a8dd25e1c58a94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_497949a5549b47c9b4beade60a2baef5",
      "placeholder": "​",
      "style": "IPY_MODEL_b2428a724ae540ab8c9de1f3407718cf",
      "value": " 3/3 [01:02&lt;00:00, 20.15s/it]"
     }
    },
    "3a1ab45685144e358379b31dba9cfd06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "474fac95e87047a1b985dc7eeb42df6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "497949a5549b47c9b4beade60a2baef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec97ad40324e6db93b8f66079fe550": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5722894bcb4341af81a742885df17624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a92e65d695346ffa1a074b54905413f",
      "placeholder": "​",
      "style": "IPY_MODEL_06f15803e64c40eda5f0d66401954020",
      "value": " 3/3 [01:01&lt;00:00, 61.65s/it]"
     }
    },
    "5847bed556e646d287567fa5fcda35be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bea8a9d3c6d442e80e9cb7da56ed49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b50ffbafbc9f4431b8a8319398481be5",
       "IPY_MODEL_00c2c360c00b433198d7794766b8fc94",
       "IPY_MODEL_5722894bcb4341af81a742885df17624"
      ],
      "layout": "IPY_MODEL_3a1ab45685144e358379b31dba9cfd06"
     }
    },
    "5f733d182671436d922614cc3c477766": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd3706c7a484221bf5623d86ce0597f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f733d182671436d922614cc3c477766",
      "placeholder": "​",
      "style": "IPY_MODEL_5847bed556e646d287567fa5fcda35be",
      "value": "generation_config.json: 100%"
     }
    },
    "68558251812240b39af7eed8d0362a39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a92e65d695346ffa1a074b54905413f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73b4ea90c09f4907b3014bacc450ea30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fddae1dad014071b9a4b8a8f314938a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a69cfa8f014240d483f9378f1fb9e68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9c691cb98da4852ad1006b11d8bcc84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b21e1a5f3e284c329b0cf3aebfc759b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f527fe0bfe33431aaf85657284bdd55a",
       "IPY_MODEL_f911848931644c318936f317489a5f3f",
       "IPY_MODEL_191ee0d353d246d6b7a8dd25e1c58a94"
      ],
      "layout": "IPY_MODEL_030b067bcdc844a3b9c879d5abb74126"
     }
    },
    "b2428a724ae540ab8c9de1f3407718cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4723b9dd89448e487cafdbeda5b243e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b50ffbafbc9f4431b8a8319398481be5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fddae1dad014071b9a4b8a8f314938a",
      "placeholder": "​",
      "style": "IPY_MODEL_a9c691cb98da4852ad1006b11d8bcc84",
      "value": "Fetching 3 files: 100%"
     }
    },
    "b5cc6a4766984da8905c083c448eb5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_474fac95e87047a1b985dc7eeb42df6c",
      "placeholder": "​",
      "style": "IPY_MODEL_73b4ea90c09f4907b3014bacc450ea30",
      "value": " 141/141 [00:00&lt;00:00, 14.7kB/s]"
     }
    },
    "bc63f284ddca4935870b39c8e35f9e1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ccb978fc3342f2a146f928c3df81da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca8284c21fe44236aabd4cb0e2ac1104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2baf8308a16400098c898ed8fb70786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ec97ad40324e6db93b8f66079fe550",
      "max": 141,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca8284c21fe44236aabd4cb0e2ac1104",
      "value": 141
     }
    },
    "f527fe0bfe33431aaf85657284bdd55a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68558251812240b39af7eed8d0362a39",
      "placeholder": "​",
      "style": "IPY_MODEL_b4723b9dd89448e487cafdbeda5b243e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "f911848931644c318936f317489a5f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1022261b5d094e8080b1286c18726397",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14667dc4dd3543868ef05786db88176c",
      "value": 3
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
