{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash-jain-1/SarcLM/blob/main/notebooks/LLaVA_Fine_tuning_with_PEFT_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocl6OYkmNUPQ",
        "outputId": "0cad8d3c-49cd-4993-8159-35e3f9087e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (1.1.10)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run only if not already installed)\n",
        "%pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0\n",
        "%pip install -U pip setuptools wheel\n",
        "%pip install bitsandbytes\n",
        "%pip install hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR9TfOdDxED6"
      },
      "outputs": [],
      "source": [
        "! pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41D2Q77rozYM"
      },
      "outputs": [],
      "source": [
        "%pip install -U ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbw5YPzBozYM"
      },
      "outputs": [],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHDLdz3fDART"
      },
      "outputs": [],
      "source": [
        "# robust_llava_loader.py\n",
        "# Loads a LLaVA-style model even when AutoModelForCausalLM doesn't recognize LlavaConfig.\n",
        "# Requires: transformers, torch, datasets, peft, trl, etc.\n",
        "# Make sure to run: pip install -U \"transformers>=4.31.0\" bitsandbytes peft trl datasets safetensors\n",
        "# if you want 4-bit quantization support (and bitsandbytes installed).\n",
        "\n",
        "import importlib\n",
        "import sys\n",
        "import traceback\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig  # if bitsandbytes present; import may fail if not installed\n",
        "from datasets import load_dataset\n",
        "\n",
        "# CONFIG\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "USE_4BIT = False  # set True only if bitsandbytes is installed & you want 4-bit quant\n",
        "DEVICE_MAP = \"auto\"\n",
        "LOW_CPU_MEM = True\n",
        "\n",
        "def print_versions():\n",
        "    print(\"torch:\", torch.__version__)\n",
        "    print(\"transformers:\", transformers.__version__)\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "        print(\"bitsandbytes:\", bnb.__version__)\n",
        "    except Exception:\n",
        "        print(\"bitsandbytes: NOT INSTALLED\")\n",
        "\n",
        "def try_load_llava_class_and_model(model_name, quant_config=None, device_map=\"auto\", low_cpu_mem=True):\n",
        "    candidate_module_paths = [\n",
        "        \"transformers.models.llava.modeling_llava\",\n",
        "        \"transformers.models.llava.modeling_llava_for_causal_lm\",\n",
        "        \"llava.modeling_llava\",\n",
        "        \"modeling_llava\",\n",
        "    ]\n",
        "    candidate_class_names = [\n",
        "        \"LlavaForCausalLM\",\n",
        "        \"LlavaModelForCausalLM\",\n",
        "        \"LlavaForConditionalGeneration\",\n",
        "        \"LlavaModel\",\n",
        "        \"LlavaForVision2Seq\",\n",
        "    ]\n",
        "\n",
        "    last_exc = None\n",
        "    for mod_path in candidate_module_paths:\n",
        "        try:\n",
        "            module = importlib.import_module(mod_path)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            continue\n",
        "\n",
        "        for cls_name in candidate_class_names:\n",
        "            ModelClass = getattr(module, cls_name, None)\n",
        "            if ModelClass is None:\n",
        "                continue\n",
        "\n",
        "            # Try strategy sequence:\n",
        "            # 1) If quant_config provided -> try direct (fast path)\n",
        "            # 2) If ValueError complaining about dispatch -> retry with llm_int8_enable_fp32_cpu_offload + device_map=\"auto\"\n",
        "            # 3) If still failing -> fallback to no-quant (float16)\n",
        "            try:\n",
        "                print(f\"Trying to load {cls_name} with device_map={device_map} (quant_config={'yes' if quant_config else 'no'})...\")\n",
        "                return _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs={})\n",
        "            except Exception as e:\n",
        "                last_exc = e\n",
        "                tb = traceback.format_exc()\n",
        "                print(f\"Initial attempt with {cls_name} failed: {e}\\n{tb}\")\n",
        "\n",
        "                # If message suggests offload, try offload route (only if quant_config not None)\n",
        "                msg = str(e).lower()\n",
        "                if quant_config is not None and (\"offload\" in msg or \"dispatched on the cpu\" in msg or \"some modules are dispatched\" in msg):\n",
        "                    try:\n",
        "                        print(\"Retrying with llm_int8_enable_fp32_cpu_offload=True and device_map='auto'...\")\n",
        "                        return ModelClass.from_pretrained(\n",
        "                            model_name,\n",
        "                            quantization_config=quant_config,\n",
        "                            device_map=\"auto\",\n",
        "                            trust_remote_code=True,\n",
        "                            low_cpu_mem_usage=low_cpu_mem,\n",
        "                            llm_int8_enable_fp32_cpu_offload=True,\n",
        "                        )\n",
        "                    except Exception as e2:\n",
        "                        last_exc = e2\n",
        "                        tb2 = traceback.format_exc()\n",
        "                        print(f\"Retry with offload failed: {e2}\\n{tb2}\")\n",
        "\n",
        "                # Final fallback: try without quantization (float16)\n",
        "                try:\n",
        "                    print(\"Retrying without quantization (float16) as a fallback...\")\n",
        "                    return ModelClass.from_pretrained(\n",
        "                        model_name,\n",
        "                        device_map=device_map,\n",
        "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                        trust_remote_code=True,\n",
        "                        low_cpu_mem_usage=low_cpu_mem,\n",
        "                    )\n",
        "                except Exception as e3:\n",
        "                    last_exc = e3\n",
        "                    tb3 = traceback.format_exc()\n",
        "                    print(f\"Fallback without quantization also failed: {e3}\\n{tb3}\")\n",
        "                    continue\n",
        "\n",
        "    raise RuntimeError(\"Tried candidate Llava classes but all failed. Last exception:\\n\" + (str(last_exc) if last_exc is not None else \"None\"))\n",
        "\n",
        "def _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs):\n",
        "    \"\"\"Helper to call from_pretrained with given kwargs and bubble exceptions.\"\"\"\n",
        "    try:\n",
        "        if quant_config is not None:\n",
        "            return ModelClass.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=quant_config,\n",
        "                device_map=device_map,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "                **extra_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            return ModelClass.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=device_map,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "                **extra_kwargs,\n",
        "            )\n",
        "    except Exception as e:\n",
        "        raise\n",
        "\n",
        "def load_model_and_tokenizer(model_name, use_4bit=True, device_map=\"auto\", low_cpu_mem=True):\n",
        "    # Load config + tokenizer\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    quant_config = None\n",
        "    if use_4bit:\n",
        "        try:\n",
        "            quant_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(\"Could not create BitsAndBytesConfig:\", e)\n",
        "            quant_config = None\n",
        "\n",
        "    # If LlavaConfig detected: try repo model classes (with fallback paths)\n",
        "    cfg_name = config.__class__.__name__.lower()\n",
        "    if \"llava\" in cfg_name:\n",
        "        model = try_load_llava_class_and_model(model_name, quant_config=quant_config, device_map=device_map, low_cpu_mem=low_cpu_mem)\n",
        "    else:\n",
        "        # generic fallback to AutoModelForCausalLM\n",
        "        if quant_config is not None:\n",
        "            try:\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    quantization_config=quant_config,\n",
        "                    device_map=device_map,\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=low_cpu_mem,\n",
        "                    llm_int8_enable_fp32_cpu_offload=True,  # safe to include\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(\"AutoModelForCausalLM with quant failed, retrying without quant:\", e)\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    device_map=device_map,\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=low_cpu_mem,\n",
        "                )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=device_map,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.config.use_cache = False\n",
        "    return model, tokenizer\n",
        "\n",
        "# Now, when you run main(), it will automatically retry with llm_int8_enable_fp32_cpu_offload if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r86yQYcGDDQp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Running in Colab. You may use Colab resources.\")\n",
        "    # Optionally: get API key from user input or environment\n",
        "    # api_key = input(\"Enter your API key: \")\n",
        "else:\n",
        "    print(\"Not running in Colab. Using local environment.\")\n",
        "    # Optionally: get API key from environment variable\n",
        "    # api_key = os.getenv(\"YOUR_API_KEY_ENV_VAR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evPygyPuUVXU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "finetune_after_script_b.py\n",
        "\n",
        "Uses robust_llava_loader.load_model_and_tokenizer() (script B output) to load the model,\n",
        "auto-detects good LoRA target_modules, and runs PEFT (LoRA) fine-tuning with trl.SFTTrainer.\n",
        "\n",
        "Usage: python finetune_after_script_b.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "# try to import helper for preparing k-bit training (peft versions vary)\n",
        "try:\n",
        "    from peft import prepare_model_for_kbit_training\n",
        "except Exception:\n",
        "    try:\n",
        "        from peft.utils import prepare_model_for_kbit_training\n",
        "    except Exception:\n",
        "        prepare_model_for_kbit_training = None\n",
        "\n",
        "# Add the src directory to the Python path so we can import robust_llava_loader\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
        "from robust_llava_loader import load_model_and_tokenizer\n",
        "\n",
        "# --------------------- USER CONFIG ---------------------\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "DATASET_PATH = \"meld_with_rationales.jsonl\"   # jsonl containing utterance, sentiment, rationale\n",
        "OUTPUT_DIR = \"./llava-peft-adapters-auto\"\n",
        "USE_4BIT_IF_AVAILABLE = True\n",
        "MAX_SEQ_LENGTH = 512\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "SAVE_STEPS = 200\n",
        "LOGGING_STEPS = 20\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# helper: create the training prompt\n",
        "def build_prompt(example):\n",
        "    return (\n",
        "        \"You are a sentiment analysis expert. Analyze the following utterance and provide \"\n",
        "        \"the sentiment along with a step-by-step rationale for your decision.\\n\\n\"\n",
        "        \"### Utterance:\\n\"\n",
        "        f\"{example.get('utterance','')}\\n\\n\"\n",
        "        \"### Analysis:\\n\"\n",
        "        f\"Sentiment: {example.get('sentiment','')}\\n\"\n",
        "        f\"Rationale: {example.get('rationale','')}\"\n",
        "    )\n",
        "\n",
        "# helper: scan model.named_modules() and choose candidate target module name substrings\n",
        "def auto_detect_target_module_names(model, prefer_text=True):\n",
        "    \"\"\"\n",
        "    Returns a list of module-name substrings to use in LoraConfig.target_modules.\n",
        "    Strategy:\n",
        "      - Collect names of submodules that look like projections (q_proj, k_proj, v_proj, out_proj, o_proj)\n",
        "      - Prefer modules under 'model' that contain tokens like 'self_attn', 'attn', 'q_proj' etc.\n",
        "      - If prefer_text=True, try to exclude modules under vision tower (module name containing 'vision' or 'vision_tower')\n",
        "    \"\"\"\n",
        "    proj_patterns = set()\n",
        "    name_list = [n for n, _ in model.named_modules()]\n",
        "\n",
        "    for n in name_list:\n",
        "        # skip top-level empty name\n",
        "        if not n:\n",
        "            continue\n",
        "        # skip vision modules if preferring text modules\n",
        "        if prefer_text and (\"vision\" in n or \"vision_tower\" in n or \"vision_model\" in n):\n",
        "            continue\n",
        "        # find typical projection/fc names in module path\n",
        "        if re.search(r\"(q_proj|k_proj|v_proj|o_proj|out_proj|gate_proj|up_proj|down_proj|fc1|fc2|mlp)\", n):\n",
        "            # extract final token (last part after '.')\n",
        "            final = n.split(\".\")[-1]\n",
        "            proj_patterns.add(final)\n",
        "    # fallback if empty\n",
        "    if not proj_patterns:\n",
        "        # default common names\n",
        "        proj_patterns = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    # keep consistent ordering and return as list\n",
        "    return sorted(list(proj_patterns))\n",
        "\n",
        "def format_and_map(example, tokenizer):\n",
        "    text = build_prompt(example)\n",
        "    eos = tokenizer.eos_token or \"\"\n",
        "    return {\"text\": text + eos}\n",
        "\n",
        "def main():\n",
        "    # decide 4-bit usage\n",
        "    use_4bit = False\n",
        "    if USE_4BIT_IF_AVAILABLE:\n",
        "        try:\n",
        "            import bitsandbytes  # noqa: F401\n",
        "            use_4bit = True\n",
        "        except Exception:\n",
        "            print(\"bitsandbytes not installed/found — running without 4-bit.\")\n",
        "\n",
        "    # 1) Load model + tokenizer (robust loader)\n",
        "    print(\"Loading model + tokenizer (robust loader)...\")\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name=BASE_MODEL_NAME, use_4bit=use_4bit)\n",
        "    print(\"Loaded model and tokenizer. Model dtype hint:\", getattr(model, \"dtype\", None))\n",
        "    model.config.use_cache = False\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Auto-detect target_modules for LoRA (based on model module names)\n",
        "    print(\"Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\")\n",
        "    detected = auto_detect_target_module_names(model, prefer_text=True)\n",
        "    print(\"Detected target-module name tokens (candidates):\", detected)\n",
        "\n",
        "    # We'll use these tokens as LoraConfig.target_modules (PEFT expects substrings)\n",
        "    target_modules = detected\n",
        "\n",
        "    # 3) Prepare model for k-bit training (if using 4-bit & helper present)\n",
        "    if use_4bit:\n",
        "        if prepare_model_for_kbit_training is not None:\n",
        "            print(\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\")\n",
        "            model = prepare_model_for_kbit_training(model)\n",
        "        else:\n",
        "            print(\"prepare_model_for_kbit_training not available in this peft version — continuing.\")\n",
        "\n",
        "    # 4) Create LoraConfig and wrap model\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    print(\"Applying LoRA with LoraConfig:\", lora_cfg)\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    print(\"PEFT/LoRA applied. Peft model keys:\", list(model.named_parameters())[:5])\n",
        "\n",
        "    # 5) Load and format dataset\n",
        "    print(\"Loading dataset from\", DATASET_PATH)\n",
        "    ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(\"Dataset size:\", len(ds))\n",
        "    # map to `text` field expected by SFTTrainer\n",
        "    ds = ds.map(lambda ex: format_and_map(ex, tokenizer), remove_columns=ds.column_names)\n",
        "\n",
        "    # 6) TrainingArguments + trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=use_4bit or (torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 12 * 1024 ** 2),\n",
        "        save_steps=SAVE_STEPS,\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # import here to avoid top-level dependency until needed\n",
        "    from trl import SFTTrainer\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=ds,\n",
        "        peft_config=lora_cfg,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        packing=False,\n",
        "    )\n",
        "\n",
        "    # 7) Dry-run: single step to validate forward/backward\n",
        "    print(\"Running a 1-step dry-run to validate training loop...\")\n",
        "    try:\n",
        "        trainer.train(max_steps=1)\n",
        "        print(\"Dry-run succeeded.\")\n",
        "    except Exception as e:\n",
        "        print(\"Dry-run failed — inspect traceback. Error:\", e)\n",
        "        raise\n",
        "\n",
        "    # 8) Full training\n",
        "    print(\"Starting full training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # 9) Save PEFT adapters\n",
        "    print(\"Saving adapters to:\", OUTPUT_DIR)\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    print(\"Saved. You can load later with PeftModel.from_pretrained(base_model, OUTPUT_DIR)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAFcBo8uVALa"
      },
      "outputs": [],
      "source": [
        "# (This cell was a duplicate of robust_llava_loader.py logic and is now removed for clarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkKd6kxXXHG6"
      },
      "outputs": [],
      "source": [
        "# (Empty cell placeholder removed for clarity)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}