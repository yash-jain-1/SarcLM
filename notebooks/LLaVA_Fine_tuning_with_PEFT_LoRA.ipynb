{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ocl6OYkmNUPQ",
    "outputId": "2f13de0e-3d2f-4248-9a55-306dc654c798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from bitsandbytes) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl.metadata (4.7 kB)\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.8 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.0/2.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.3/2.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.8/2.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.8 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.4/2.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.6/2.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 1.1 MB/s  0:00:02\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run only if not already installed)\n",
    "%pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install bitsandbytes\n",
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DR9TfOdDxED6",
    "outputId": "9941ac84-c2b5-4279-e394-e41e77097c6c"
   },
   "outputs": [],
   "source": [
    "! pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\yashj\\downloads\\capstone\\sarclm\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5bea8a9d3c6d442e80e9cb7da56ed49c",
      "b50ffbafbc9f4431b8a8319398481be5",
      "00c2c360c00b433198d7794766b8fc94",
      "5722894bcb4341af81a742885df17624",
      "3a1ab45685144e358379b31dba9cfd06",
      "7fddae1dad014071b9a4b8a8f314938a",
      "a9c691cb98da4852ad1006b11d8bcc84",
      "bc63f284ddca4935870b39c8e35f9e1d",
      "c5ccb978fc3342f2a146f928c3df81da",
      "6a92e65d695346ffa1a074b54905413f",
      "06f15803e64c40eda5f0d66401954020",
      "b21e1a5f3e284c329b0cf3aebfc759b1",
      "f527fe0bfe33431aaf85657284bdd55a",
      "f911848931644c318936f317489a5f3f",
      "191ee0d353d246d6b7a8dd25e1c58a94",
      "030b067bcdc844a3b9c879d5abb74126",
      "68558251812240b39af7eed8d0362a39",
      "b4723b9dd89448e487cafdbeda5b243e",
      "1022261b5d094e8080b1286c18726397",
      "14667dc4dd3543868ef05786db88176c",
      "497949a5549b47c9b4beade60a2baef5",
      "b2428a724ae540ab8c9de1f3407718cf",
      "122794ba89c0404486853ead0ffc8415",
      "5fd3706c7a484221bf5623d86ce0597f",
      "f2baf8308a16400098c898ed8fb70786",
      "b5cc6a4766984da8905c083c448eb5bc",
      "a69cfa8f014240d483f9378f1fb9e68a",
      "5f733d182671436d922614cc3c477766",
      "5847bed556e646d287567fa5fcda35be",
      "53ec97ad40324e6db93b8f66079fe550",
      "ca8284c21fe44236aabd4cb0e2ac1104",
      "474fac95e87047a1b985dc7eeb42df6c",
      "73b4ea90c09f4907b3014bacc450ea30"
     ]
    },
    "id": "LHDLdz3fDART",
    "outputId": "1de44e9b-c3d5-4c79-fbee-333fae5ef9ea"
   },
   "outputs": [],
   "source": [
    "# robust_llava_loader.py\n",
    "# Loads a LLaVA-style model even when AutoModelForCausalLM doesn't recognize LlavaConfig.\n",
    "# Requires: transformers, torch, datasets, peft, trl, etc.\n",
    "# Make sure to run: pip install -U \"transformers>=4.31.0\" bitsandbytes peft trl datasets safetensors\n",
    "# if you want 4-bit quantization support (and bitsandbytes installed).\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import traceback\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig  # if bitsandbytes present; import may fail if not installed\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CONFIG\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "USE_4BIT = False  # set True only if bitsandbytes is installed & you want 4-bit quant\n",
    "DEVICE_MAP = \"auto\"\n",
    "LOW_CPU_MEM = True\n",
    "\n",
    "def print_versions():\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        print(\"bitsandbytes:\", bnb.__version__)\n",
    "    except Exception:\n",
    "        print(\"bitsandbytes: NOT INSTALLED\")\n",
    "\n",
    "def try_load_llava_class_and_model(model_name, quant_config=None, device_map=\"auto\", low_cpu_mem=True):\n",
    "    candidate_module_paths = [\n",
    "        \"transformers.models.llava.modeling_llava\",\n",
    "        \"transformers.models.llava.modeling_llava_for_causal_lm\",\n",
    "        \"llava.modeling_llava\",\n",
    "        \"modeling_llava\",\n",
    "    ]\n",
    "    candidate_class_names = [\n",
    "        \"LlavaForCausalLM\",\n",
    "        \"LlavaModelForCausalLM\",\n",
    "        \"LlavaForConditionalGeneration\",\n",
    "        \"LlavaModel\",\n",
    "        \"LlavaForVision2Seq\",\n",
    "    ]\n",
    "\n",
    "    last_exc = None\n",
    "    for mod_path in candidate_module_paths:\n",
    "        try:\n",
    "            module = importlib.import_module(mod_path)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            continue\n",
    "\n",
    "        for cls_name in candidate_class_names:\n",
    "            ModelClass = getattr(module, cls_name, None)\n",
    "            if ModelClass is None:\n",
    "                continue\n",
    "\n",
    "            # Try strategy sequence:\n",
    "            # 1) If quant_config provided -> try direct (fast path)\n",
    "            # 2) If ValueError complaining about dispatch -> retry with llm_int8_enable_fp32_cpu_offload + device_map=\"auto\"\n",
    "            # 3) If still failing -> fallback to no-quant (float16)\n",
    "            try:\n",
    "                print(f\"Trying to load {cls_name} with device_map={device_map} (quant_config={'yes' if quant_config else 'no'})...\")\n",
    "                return _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs={})\n",
    "            except Exception as e:\n",
    "                last_exc = e\n",
    "                tb = traceback.format_exc()\n",
    "                print(f\"Initial attempt with {cls_name} failed: {e}\\n{tb}\")\n",
    "\n",
    "                # If message suggests offload, try offload route (only if quant_config not None)\n",
    "                msg = str(e).lower()\n",
    "                if quant_config is not None and (\"offload\" in msg or \"dispatched on the cpu\" in msg or \"some modules are dispatched\" in msg):\n",
    "                    try:\n",
    "                        print(\"Retrying with llm_int8_enable_fp32_cpu_offload=True and device_map='auto'...\")\n",
    "                        return ModelClass.from_pretrained(\n",
    "                            model_name,\n",
    "                            quantization_config=quant_config,\n",
    "                            device_map=\"auto\",\n",
    "                            trust_remote_code=True,\n",
    "                            low_cpu_mem_usage=low_cpu_mem,\n",
    "                            llm_int8_enable_fp32_cpu_offload=True,\n",
    "                        )\n",
    "                    except Exception as e2:\n",
    "                        last_exc = e2\n",
    "                        tb2 = traceback.format_exc()\n",
    "                        print(f\"Retry with offload failed: {e2}\\n{tb2}\")\n",
    "\n",
    "                # Final fallback: try without quantization (float16)\n",
    "                try:\n",
    "                    print(\"Retrying without quantization (float16) as a fallback...\")\n",
    "                    return ModelClass.from_pretrained(\n",
    "                        model_name,\n",
    "                        device_map=device_map,\n",
    "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                        trust_remote_code=True,\n",
    "                        low_cpu_mem_usage=low_cpu_mem,\n",
    "                    )\n",
    "                except Exception as e3:\n",
    "                    last_exc = e3\n",
    "                    tb3 = traceback.format_exc()\n",
    "                    print(f\"Fallback without quantization also failed: {e3}\\n{tb3}\")\n",
    "                    continue\n",
    "\n",
    "    raise RuntimeError(\"Tried candidate Llava classes but all failed. Last exception:\\n\" + (str(last_exc) if last_exc is not None else \"None\"))\n",
    "\n",
    "def _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs):\n",
    "    \"\"\"Helper to call from_pretrained with given kwargs and bubble exceptions.\"\"\"\n",
    "    try:\n",
    "        if quant_config is not None:\n",
    "            return ModelClass.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=device_map,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "                **extra_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            return ModelClass.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "                **extra_kwargs,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def load_model_and_tokenizer(model_name, use_4bit=True, device_map=\"auto\", low_cpu_mem=True):\n",
    "    # Load config + tokenizer\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    quant_config = None\n",
    "    if use_4bit:\n",
    "        try:\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Could not create BitsAndBytesConfig:\", e)\n",
    "            quant_config = None\n",
    "\n",
    "    # If LlavaConfig detected: try repo model classes (with fallback paths)\n",
    "    cfg_name = config.__class__.__name__.lower()\n",
    "    if \"llava\" in cfg_name:\n",
    "        model = try_load_llava_class_and_model(model_name, quant_config=quant_config, device_map=device_map, low_cpu_mem=low_cpu_mem)\n",
    "    else:\n",
    "        # generic fallback to AutoModelForCausalLM\n",
    "        if quant_config is not None:\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=quant_config,\n",
    "                    device_map=device_map,\n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=low_cpu_mem,\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,  # safe to include\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"AutoModelForCausalLM with quant failed, retrying without quant:\", e)\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=device_map,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=low_cpu_mem,\n",
    "                )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=low_cpu_mem,\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "\n",
    "# Now, when you run main(), it will automatically retry with llm_int8_enable_fp32_cpu_offload if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r86yQYcGDDQp",
    "outputId": "2e2bd0c4-2871-468a-ab13-37f901f75c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab. Using local environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Running in Colab. You may use Colab resources.\")\n",
    "    # Optionally: get API key from user input or environment\n",
    "    # api_key = input(\"Enter your API key: \")\n",
    "else:\n",
    "    print(\"Not running in Colab. Using local environment.\")\n",
    "    # Optionally: get API key from environment variable\n",
    "    # api_key = os.getenv(\"YOUR_API_KEY_ENV_VAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "evPygyPuUVXU",
    "outputId": "7108007a-5c04-46fc-b357-0b35ae1b0a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model + tokenizer (robust loader)...\n",
      "Trying to load LlavaForConditionalGeneration with device_map=auto (quant_config=yes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bfa9d641cf412aa5b670d3597f8c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945b48e1adfb49eb8e96343059496e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad456633458c48bfab6f8ea6eb0331a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969646bba54f4da7984d363320ebe9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "finetune_after_script_b.py\n",
    "\n",
    "Uses robust_llava_loader.load_model_and_tokenizer() (script B output) to load the model,\n",
    "auto-detects good LoRA target_modules, and runs PEFT (LoRA) fine-tuning with trl.SFTTrainer.\n",
    "\n",
    "Usage: python finetune_after_script_b.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# try to import helper for preparing k-bit training (peft versions vary)\n",
    "try:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "except Exception:\n",
    "    try:\n",
    "        from peft.utils import prepare_model_for_kbit_training\n",
    "    except Exception:\n",
    "        prepare_model_for_kbit_training = None\n",
    "\n",
    "# Add the src directory to the Python path so we can import robust_llava_loader\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
    "from robust_llava_loader import load_model_and_tokenizer\n",
    "\n",
    "# --------------------- USER CONFIG ---------------------\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "DATASET_PATH = \"meld_with_rationales.jsonl\"   # jsonl containing utterance, sentiment, rationale\n",
    "OUTPUT_DIR = \"./llava-peft-adapters-auto\"\n",
    "USE_4BIT_IF_AVAILABLE = True\n",
    "MAX_SEQ_LENGTH = 512\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "SAVE_STEPS = 200\n",
    "LOGGING_STEPS = 20\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# helper: create the training prompt\n",
    "def build_prompt(example):\n",
    "    return (\n",
    "        \"You are a sentiment analysis expert. Analyze the following utterance and provide \"\n",
    "        \"the sentiment along with a step-by-step rationale for your decision.\\n\\n\"\n",
    "        \"### Utterance:\\n\"\n",
    "        f\"{example.get('utterance','')}\\n\\n\"\n",
    "        \"### Analysis:\\n\"\n",
    "        f\"Sentiment: {example.get('sentiment','')}\\n\"\n",
    "        f\"Rationale: {example.get('rationale','')}\"\n",
    "    )\n",
    "\n",
    "# helper: scan model.named_modules() and choose candidate target module name substrings\n",
    "def auto_detect_target_module_names(model, prefer_text=True):\n",
    "    \"\"\"\n",
    "    Returns a list of module-name substrings to use in LoraConfig.target_modules.\n",
    "    Strategy:\n",
    "      - Collect names of submodules that look like projections (q_proj, k_proj, v_proj, out_proj, o_proj)\n",
    "      - Prefer modules under 'model' that contain tokens like 'self_attn', 'attn', 'q_proj' etc.\n",
    "      - If prefer_text=True, try to exclude modules under vision tower (module name containing 'vision' or 'vision_tower')\n",
    "    \"\"\"\n",
    "    proj_patterns = set()\n",
    "    name_list = [n for n, _ in model.named_modules()]\n",
    "\n",
    "    for n in name_list:\n",
    "        # skip top-level empty name\n",
    "        if not n:\n",
    "            continue\n",
    "        # skip vision modules if preferring text modules\n",
    "        if prefer_text and (\"vision\" in n or \"vision_tower\" in n or \"vision_model\" in n):\n",
    "            continue\n",
    "        # find typical projection/fc names in module path\n",
    "        if re.search(r\"(q_proj|k_proj|v_proj|o_proj|out_proj|gate_proj|up_proj|down_proj|fc1|fc2|mlp)\", n):\n",
    "            # extract final token (last part after '.')\n",
    "            final = n.split(\".\")[-1]\n",
    "            proj_patterns.add(final)\n",
    "    # fallback if empty\n",
    "    if not proj_patterns:\n",
    "        # default common names\n",
    "        proj_patterns = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
    "    # keep consistent ordering and return as list\n",
    "    return sorted(list(proj_patterns))\n",
    "\n",
    "def format_and_map(example, tokenizer):\n",
    "    text = build_prompt(example)\n",
    "    eos = tokenizer.eos_token or \"\"\n",
    "    return {\"text\": text + eos}\n",
    "\n",
    "def main():\n",
    "    # decide 4-bit usage\n",
    "    use_4bit = False\n",
    "    if USE_4BIT_IF_AVAILABLE:\n",
    "        try:\n",
    "            import bitsandbytes  # noqa: F401\n",
    "            use_4bit = True\n",
    "        except Exception:\n",
    "            print(\"bitsandbytes not installed/found — running without 4-bit.\")\n",
    "\n",
    "    # 1) Load model + tokenizer (robust loader)\n",
    "    print(\"Loading model + tokenizer (robust loader)...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name=BASE_MODEL_NAME, use_4bit=use_4bit)\n",
    "    print(\"Loaded model and tokenizer. Model dtype hint:\", getattr(model, \"dtype\", None))\n",
    "    model.config.use_cache = False\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Auto-detect target_modules for LoRA (based on model module names)\n",
    "    print(\"Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\")\n",
    "    detected = auto_detect_target_module_names(model, prefer_text=True)\n",
    "    print(\"Detected target-module name tokens (candidates):\", detected)\n",
    "\n",
    "    # We'll use these tokens as LoraConfig.target_modules (PEFT expects substrings)\n",
    "    target_modules = detected\n",
    "\n",
    "    # 3) Prepare model for k-bit training (if using 4-bit & helper present)\n",
    "    if use_4bit:\n",
    "        if prepare_model_for_kbit_training is not None:\n",
    "            print(\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\")\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        else:\n",
    "            print(\"prepare_model_for_kbit_training not available in this peft version — continuing.\")\n",
    "\n",
    "    # 4) Create LoraConfig and wrap model\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    print(\"Applying LoRA with LoraConfig:\", lora_cfg)\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    print(\"PEFT/LoRA applied. Peft model keys:\", list(model.named_parameters())[:5])\n",
    "\n",
    "    # 5) Load and format dataset\n",
    "    print(\"Loading dataset from\", DATASET_PATH)\n",
    "    ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "    print(\"Dataset size:\", len(ds))\n",
    "    # map to `text` field expected by SFTTrainer\n",
    "    ds = ds.map(lambda ex: format_and_map(ex, tokenizer), remove_columns=ds.column_names)\n",
    "\n",
    "    # 6) TrainingArguments + trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=use_4bit or (torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 12 * 1024 ** 2),\n",
    "        save_steps=SAVE_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        save_total_limit=3,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # import here to avoid top-level dependency until needed\n",
    "    from trl import SFTTrainer\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ds,\n",
    "        peft_config=lora_cfg,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    # 7) Dry-run: single step to validate forward/backward\n",
    "    print(\"Running a 1-step dry-run to validate training loop...\")\n",
    "    try:\n",
    "        trainer.train(max_steps=1)\n",
    "        print(\"Dry-run succeeded.\")\n",
    "    except Exception as e:\n",
    "        print(\"Dry-run failed — inspect traceback. Error:\", e)\n",
    "        raise\n",
    "\n",
    "    # 8) Full training\n",
    "    print(\"Starting full training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # 9) Save PEFT adapters\n",
    "    print(\"Saving adapters to:\", OUTPUT_DIR)\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(\"Saved. You can load later with PeftModel.from_pretrained(base_model, OUTPUT_DIR)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAFcBo8uVALa"
   },
   "outputs": [],
   "source": [
    "# (This cell was a duplicate of robust_llava_loader.py logic and is now removed for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkKd6kxXXHG6"
   },
   "outputs": [],
   "source": [
    "# (Empty cell placeholder removed for clarity)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c2c360c00b433198d7794766b8fc94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc63f284ddca4935870b39c8e35f9e1d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5ccb978fc3342f2a146f928c3df81da",
      "value": 3
     }
    },
    "030b067bcdc844a3b9c879d5abb74126": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06f15803e64c40eda5f0d66401954020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1022261b5d094e8080b1286c18726397": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "122794ba89c0404486853ead0ffc8415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5fd3706c7a484221bf5623d86ce0597f",
       "IPY_MODEL_f2baf8308a16400098c898ed8fb70786",
       "IPY_MODEL_b5cc6a4766984da8905c083c448eb5bc"
      ],
      "layout": "IPY_MODEL_a69cfa8f014240d483f9378f1fb9e68a"
     }
    },
    "14667dc4dd3543868ef05786db88176c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "191ee0d353d246d6b7a8dd25e1c58a94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_497949a5549b47c9b4beade60a2baef5",
      "placeholder": "​",
      "style": "IPY_MODEL_b2428a724ae540ab8c9de1f3407718cf",
      "value": " 3/3 [01:02&lt;00:00, 20.15s/it]"
     }
    },
    "3a1ab45685144e358379b31dba9cfd06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "474fac95e87047a1b985dc7eeb42df6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "497949a5549b47c9b4beade60a2baef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec97ad40324e6db93b8f66079fe550": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5722894bcb4341af81a742885df17624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a92e65d695346ffa1a074b54905413f",
      "placeholder": "​",
      "style": "IPY_MODEL_06f15803e64c40eda5f0d66401954020",
      "value": " 3/3 [01:01&lt;00:00, 61.65s/it]"
     }
    },
    "5847bed556e646d287567fa5fcda35be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bea8a9d3c6d442e80e9cb7da56ed49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b50ffbafbc9f4431b8a8319398481be5",
       "IPY_MODEL_00c2c360c00b433198d7794766b8fc94",
       "IPY_MODEL_5722894bcb4341af81a742885df17624"
      ],
      "layout": "IPY_MODEL_3a1ab45685144e358379b31dba9cfd06"
     }
    },
    "5f733d182671436d922614cc3c477766": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd3706c7a484221bf5623d86ce0597f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f733d182671436d922614cc3c477766",
      "placeholder": "​",
      "style": "IPY_MODEL_5847bed556e646d287567fa5fcda35be",
      "value": "generation_config.json: 100%"
     }
    },
    "68558251812240b39af7eed8d0362a39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a92e65d695346ffa1a074b54905413f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73b4ea90c09f4907b3014bacc450ea30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fddae1dad014071b9a4b8a8f314938a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a69cfa8f014240d483f9378f1fb9e68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9c691cb98da4852ad1006b11d8bcc84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b21e1a5f3e284c329b0cf3aebfc759b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f527fe0bfe33431aaf85657284bdd55a",
       "IPY_MODEL_f911848931644c318936f317489a5f3f",
       "IPY_MODEL_191ee0d353d246d6b7a8dd25e1c58a94"
      ],
      "layout": "IPY_MODEL_030b067bcdc844a3b9c879d5abb74126"
     }
    },
    "b2428a724ae540ab8c9de1f3407718cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4723b9dd89448e487cafdbeda5b243e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b50ffbafbc9f4431b8a8319398481be5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fddae1dad014071b9a4b8a8f314938a",
      "placeholder": "​",
      "style": "IPY_MODEL_a9c691cb98da4852ad1006b11d8bcc84",
      "value": "Fetching 3 files: 100%"
     }
    },
    "b5cc6a4766984da8905c083c448eb5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_474fac95e87047a1b985dc7eeb42df6c",
      "placeholder": "​",
      "style": "IPY_MODEL_73b4ea90c09f4907b3014bacc450ea30",
      "value": " 141/141 [00:00&lt;00:00, 14.7kB/s]"
     }
    },
    "bc63f284ddca4935870b39c8e35f9e1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ccb978fc3342f2a146f928c3df81da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca8284c21fe44236aabd4cb0e2ac1104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2baf8308a16400098c898ed8fb70786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ec97ad40324e6db93b8f66079fe550",
      "max": 141,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca8284c21fe44236aabd4cb0e2ac1104",
      "value": 141
     }
    },
    "f527fe0bfe33431aaf85657284bdd55a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68558251812240b39af7eed8d0362a39",
      "placeholder": "​",
      "style": "IPY_MODEL_b4723b9dd89448e487cafdbeda5b243e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "f911848931644c318936f317489a5f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1022261b5d094e8080b1286c18726397",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14667dc4dd3543868ef05786db88176c",
      "value": 3
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
