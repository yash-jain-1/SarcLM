{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash-jain-1/SarcLM/blob/main/notebooks/LLaVA_Fine_tuning_with_PEFT_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ocl6OYkmNUPQ",
        "outputId": "6c20c83b-1ae4-4d1c-a349-0acb8b0822ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.2 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "003b0233631c436baf3a063e3c609a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (1.1.10)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run only if not already installed)\n",
        "%pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0\n",
        "%pip install -U pip setuptools wheel\n",
        "%pip install bitsandbytes\n",
        "%pip install hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DR9TfOdDxED6"
      },
      "outputs": [],
      "source": [
        "! pip install -q -U transformers datasets accelerate peft bitsandbytes trl pyarrow==19.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "41D2Q77rozYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce3c327-dd62-4648-dd7b-bf784e70d398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (7.7.1)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets)\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (80.9.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jedi, comm, ipywidgets\n",
            "\u001b[2K  Attempting uninstall: widgetsnbextension\n",
            "\u001b[2K    Found existing installation: widgetsnbextension 3.6.10\n",
            "\u001b[2K    Uninstalling widgetsnbextension-3.6.10:\n",
            "\u001b[2K      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "\u001b[2K  Attempting uninstall: ipywidgets\n",
            "\u001b[2K    Found existing installation: ipywidgets 7.7.1\n",
            "\u001b[2K    Uninstalling ipywidgets-7.7.1:\n",
            "\u001b[2K      Successfully uninstalled ipywidgets-7.7.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [ipywidgets]\n",
            "\u001b[1A\u001b[2KSuccessfully installed comm-0.2.3 ipywidgets-8.1.7 jedi-0.19.2 widgetsnbextension-4.0.14\n"
          ]
        }
      ],
      "source": [
        "%pip install -U ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zbw5YPzBozYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c350e23-8920-4e1d-a8fb-41f7b7fa5acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LHDLdz3fDART"
      },
      "outputs": [],
      "source": [
        "# robust_llava_loader.py\n",
        "# Loads a LLaVA-style model even when AutoModelForCausalLM doesn't recognize LlavaConfig.\n",
        "# Requires: transformers, torch, datasets, peft, trl, etc.\n",
        "# Make sure to run: pip install -U \"transformers>=4.31.0\" bitsandbytes peft trl datasets safetensors\n",
        "# if you want 4-bit quantization support (and bitsandbytes installed).\n",
        "\n",
        "import importlib\n",
        "import sys\n",
        "import traceback\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig  # if bitsandbytes present; import may fail if not installed\n",
        "from datasets import load_dataset\n",
        "\n",
        "# CONFIG\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "USE_4BIT = False  # set True only if bitsandbytes is installed & you want 4-bit quant\n",
        "DEVICE_MAP = \"auto\"\n",
        "LOW_CPU_MEM = True\n",
        "\n",
        "def print_versions():\n",
        "    print(\"torch:\", torch.__version__)\n",
        "    print(\"transformers:\", transformers.__version__)\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "        print(\"bitsandbytes:\", bnb.__version__)\n",
        "    except Exception:\n",
        "        print(\"bitsandbytes: NOT INSTALLED\")\n",
        "\n",
        "def try_load_llava_class_and_model(model_name, quant_config=None, device_map=\"auto\", low_cpu_mem=True):\n",
        "    candidate_module_paths = [\n",
        "        \"transformers.models.llava.modeling_llava\",\n",
        "        \"transformers.models.llava.modeling_llava_for_causal_lm\",\n",
        "        \"llava.modeling_llava\",\n",
        "        \"modeling_llava\",\n",
        "    ]\n",
        "    candidate_class_names = [\n",
        "        \"LlavaForCausalLM\",\n",
        "        \"LlavaModelForCausalLM\",\n",
        "        \"LlavaForConditionalGeneration\",\n",
        "        \"LlavaModel\",\n",
        "        \"LlavaForVision2Seq\",\n",
        "    ]\n",
        "\n",
        "    last_exc = None\n",
        "    for mod_path in candidate_module_paths:\n",
        "        try:\n",
        "            module = importlib.import_module(mod_path)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            continue\n",
        "\n",
        "        for cls_name in candidate_class_names:\n",
        "            ModelClass = getattr(module, cls_name, None)\n",
        "            if ModelClass is None:\n",
        "                continue\n",
        "\n",
        "            # Try strategy sequence:\n",
        "            # 1) If quant_config provided -> try direct (fast path)\n",
        "            # 2) If ValueError complaining about dispatch -> retry with llm_int8_enable_fp32_cpu_offload + device_map=\"auto\"\n",
        "            # 3) If still failing -> fallback to no-quant (float16)\n",
        "            try:\n",
        "                print(f\"Trying to load {cls_name} with device_map={device_map} (quant_config={'yes' if quant_config else 'no'})...\")\n",
        "                return _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs={})\n",
        "            except Exception as e:\n",
        "                last_exc = e\n",
        "                tb = traceback.format_exc()\n",
        "                print(f\"Initial attempt with {cls_name} failed: {e}\\n{tb}\")\n",
        "\n",
        "                # If message suggests offload, try offload route (only if quant_config not None)\n",
        "                msg = str(e).lower()\n",
        "                if quant_config is not None and (\"offload\" in msg or \"dispatched on the cpu\" in msg or \"some modules are dispatched\" in msg):\n",
        "                    try:\n",
        "                        print(\"Retrying with llm_int8_enable_fp32_cpu_offload=True and device_map='auto'...\")\n",
        "                        return ModelClass.from_pretrained(\n",
        "                            model_name,\n",
        "                            quantization_config=quant_config,\n",
        "                            device_map=\"auto\",\n",
        "                            trust_remote_code=True,\n",
        "                            low_cpu_mem_usage=low_cpu_mem,\n",
        "                            llm_int8_enable_fp32_cpu_offload=True,\n",
        "                        )\n",
        "                    except Exception as e2:\n",
        "                        last_exc = e2\n",
        "                        tb2 = traceback.format_exc()\n",
        "                        print(f\"Retry with offload failed: {e2}\\n{tb2}\")\n",
        "\n",
        "                # Final fallback: try without quantization (float16)\n",
        "                try:\n",
        "                    print(\"Retrying without quantization (float16) as a fallback...\")\n",
        "                    return ModelClass.from_pretrained(\n",
        "                        model_name,\n",
        "                        device_map=device_map,\n",
        "                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                        trust_remote_code=True,\n",
        "                        low_cpu_mem_usage=low_cpu_mem,\n",
        "                    )\n",
        "                except Exception as e3:\n",
        "                    last_exc = e3\n",
        "                    tb3 = traceback.format_exc()\n",
        "                    print(f\"Fallback without quantization also failed: {e3}\\n{tb3}\")\n",
        "                    continue\n",
        "\n",
        "    raise RuntimeError(\"Tried candidate Llava classes but all failed. Last exception:\\n\" + (str(last_exc) if last_exc is not None else \"None\"))\n",
        "\n",
        "def _attempt_from_pretrained(ModelClass, model_name, quant_config, device_map, low_cpu_mem, extra_kwargs):\n",
        "    \"\"\"Helper to call from_pretrained with given kwargs and bubble exceptions.\"\"\"\n",
        "    try:\n",
        "        if quant_config is not None:\n",
        "            return ModelClass.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=quant_config,\n",
        "                device_map=device_map,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "                **extra_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            return ModelClass.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=device_map,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "                **extra_kwargs,\n",
        "            )\n",
        "    except Exception as e:\n",
        "        raise\n",
        "\n",
        "def load_model_and_tokenizer(model_name, use_4bit=True, device_map=\"auto\", low_cpu_mem=True):\n",
        "    # Load config + tokenizer\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    quant_config = None\n",
        "    if use_4bit:\n",
        "        try:\n",
        "            quant_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(\"Could not create BitsAndBytesConfig:\", e)\n",
        "            quant_config = None\n",
        "\n",
        "    # If LlavaConfig detected: try repo model classes (with fallback paths)\n",
        "    cfg_name = config.__class__.__name__.lower()\n",
        "    if \"llava\" in cfg_name:\n",
        "        model = try_load_llava_class_and_model(model_name, quant_config=quant_config, device_map=device_map, low_cpu_mem=low_cpu_mem)\n",
        "    else:\n",
        "        # generic fallback to AutoModelForCausalLM\n",
        "        if quant_config is not None:\n",
        "            try:\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    quantization_config=quant_config,\n",
        "                    device_map=device_map,\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=low_cpu_mem,\n",
        "                    llm_int8_enable_fp32_cpu_offload=True,  # safe to include\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(\"AutoModelForCausalLM with quant failed, retrying without quant:\", e)\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    device_map=device_map,\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    trust_remote_code=True,\n",
        "                    low_cpu_mem_usage=low_cpu_mem,\n",
        "                )\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=device_map,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=low_cpu_mem,\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.config.use_cache = False\n",
        "    return model, tokenizer\n",
        "\n",
        "# Now, when you run main(), it will automatically retry with llm_int8_enable_fp32_cpu_offload if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r86yQYcGDDQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29cdabf-abe0-4693-8e64-26a050f3bcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Running in Colab. You may use Colab resources.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Running in Colab. You may use Colab resources.\")\n",
        "    # Optionally: get API key from user input or environment\n",
        "    # api_key = input(\"Enter your API key: \")\n",
        "else:\n",
        "    print(\"Not running in Colab. Using local environment.\")\n",
        "    # Optionally: get API key from environment variable\n",
        "    # api_key = os.getenv(\"YOUR_API_KEY_ENV_VAR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evPygyPuUVXU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f5613a6c3ccd4077ba3b453d07fdda93",
            "0a9e5dcada3f439d85db8f5fcb013994",
            "8b63505faecd4cb3bf6740dd7c3d7cfb",
            "2f34c26d434546e5a9788e22b6556187",
            "6b7986ae63c940f98d85183bf51d1835",
            "9ad458f171e34f0ea30b1f7940dd3728",
            "e2d98cb4daa94d81aed7d5c6107e39ea",
            "e29c7397e341446b84390c1631351c1a",
            "84e099d27c5c49f19a81a23d1db502aa",
            "75675b02d22f40fbbb27c2cc212522b6",
            "69359fafe2b44b85978372ffcd8e4795"
          ]
        },
        "outputId": "18dc72a8-5c71-4989-cb24-9909bcc8e087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model + tokenizer (robust loader)...\n",
            "Trying to load LlavaForConditionalGeneration with device_map=auto (quant_config=yes)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5613a6c3ccd4077ba3b453d07fdda93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model and tokenizer. Model dtype hint: torch.float16\n",
            "Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\n",
            "Detected target-module name tokens (candidates): ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
            "Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\n",
            "Applying LoRA with LoraConfig: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules={'o_proj', 'k_proj', 'up_proj', 'gate_proj', 'q_proj', 'v_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)\n",
            "PEFT/LoRA applied. Peft model keys: [('base_model.model.model.vision_tower.vision_model.embeddings.class_embedding', Parameter containing:\n",
            "tensor([ 0.0137,  0.2371, -0.1284,  ...,  0.0171, -0.3352, -0.2383],\n",
            "       device='cuda:0')), ('base_model.model.model.vision_tower.vision_model.embeddings.patch_embedding.weight', Parameter containing:\n",
            "tensor([[[[ 2.4582e-02,  1.0330e-02,  7.1983e-03,  ...,  2.3529e-02,\n",
            "            2.1957e-02,  5.0011e-03],\n",
            "          [ 1.2756e-02, -6.1417e-03, -4.8676e-03,  ...,  1.6586e-02,\n",
            "            7.5417e-03, -1.2230e-02],\n",
            "          [ 9.9945e-03,  2.1954e-03,  2.3632e-03,  ...,  6.1607e-03,\n",
            "            5.3940e-03, -1.2283e-02],\n",
            "          ...,\n",
            "          [-9.9182e-03, -5.7507e-04, -5.5237e-03,  ..., -1.8646e-02,\n",
            "           -2.3239e-02, -2.4017e-02],\n",
            "          [-3.8624e-03, -1.1452e-02, -1.3794e-02,  ..., -7.1030e-03,\n",
            "            1.5745e-03, -4.1771e-03],\n",
            "          [-2.1545e-02, -4.2786e-02, -2.9480e-02,  ..., -4.9706e-03,\n",
            "            4.0169e-03, -6.7177e-03]],\n",
            "\n",
            "         [[ 1.4839e-02, -5.3024e-03, -1.2367e-02,  ...,  2.5696e-02,\n",
            "            2.4185e-02,  5.8136e-03],\n",
            "          [ 8.6403e-04, -2.4017e-02, -2.6428e-02,  ...,  1.4999e-02,\n",
            "            5.4970e-03, -1.4351e-02],\n",
            "          [ 4.8218e-03, -7.4463e-03, -9.7122e-03,  ...,  8.0948e-03,\n",
            "            5.5618e-03, -1.2390e-02],\n",
            "          ...,\n",
            "          [-4.4479e-03,  5.3673e-03, -1.1482e-03,  ...,  3.6259e-03,\n",
            "            3.9816e-04, -5.1346e-03],\n",
            "          [-3.6697e-03, -1.2306e-02, -1.4244e-02,  ...,  1.4938e-02,\n",
            "            2.2552e-02,  1.2917e-02],\n",
            "          [-2.8137e-02, -4.9500e-02, -3.0746e-02,  ...,  1.2810e-02,\n",
            "            1.8158e-02,  9.2745e-04]],\n",
            "\n",
            "         [[ 1.5839e-02, -2.0618e-03, -4.8103e-03,  ...,  1.6174e-02,\n",
            "            1.1757e-02,  5.7259e-03],\n",
            "          [ 1.1625e-03, -2.1133e-02, -2.4445e-02,  ...,  6.3782e-03,\n",
            "           -4.2191e-03, -7.1831e-03],\n",
            "          [ 5.0583e-03, -1.0559e-02, -1.4809e-02,  ...,  1.3199e-03,\n",
            "            1.3041e-04, -7.3051e-03],\n",
            "          ...,\n",
            "          [ 2.0676e-03,  2.0599e-03, -8.4915e-03,  ..., -9.9335e-03,\n",
            "           -9.8267e-03, -6.7825e-03],\n",
            "          [ 7.3013e-03, -3.9787e-03, -9.5062e-03,  ..., -3.2978e-03,\n",
            "            6.3400e-03,  5.3177e-03],\n",
            "          [-1.1139e-02, -2.7283e-02, -1.5518e-02,  ...,  1.6184e-03,\n",
            "            6.5193e-03,  2.9850e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5656e-02,  2.6489e-02,  6.7024e-03,  ...,  8.8120e-03,\n",
            "           -8.4381e-03,  2.0447e-02],\n",
            "          [-1.8280e-02, -6.8855e-03, -1.5854e-02,  ...,  3.4313e-03,\n",
            "            3.5906e-04,  1.5945e-02],\n",
            "          [-2.4139e-02, -1.1620e-02,  2.1858e-03,  ..., -2.5436e-02,\n",
            "           -2.7924e-02, -3.2177e-03],\n",
            "          ...,\n",
            "          [ 7.2670e-03, -7.0534e-03,  1.4343e-02,  ...,  1.4709e-02,\n",
            "           -5.4741e-03, -5.9624e-03],\n",
            "          [ 6.3133e-03, -8.2016e-03,  4.0527e-02,  ..., -3.4785e-04,\n",
            "           -2.7969e-02, -1.8875e-02],\n",
            "          [ 1.6861e-02, -1.1154e-02,  6.0089e-02,  ..., -4.8828e-04,\n",
            "            7.1335e-03,  4.6478e-02]],\n",
            "\n",
            "         [[ 2.1027e-02,  3.3264e-02,  1.0895e-02,  ...,  6.6605e-03,\n",
            "           -1.1604e-02,  1.7593e-02],\n",
            "          [-1.4885e-02, -6.6757e-04, -9.0332e-03,  ...,  4.8904e-03,\n",
            "            3.4976e-04,  1.4732e-02],\n",
            "          [-2.2797e-02, -6.2103e-03,  1.1116e-02,  ..., -2.8809e-02,\n",
            "           -3.2684e-02, -7.0076e-03],\n",
            "          ...,\n",
            "          [ 5.2643e-03, -1.2299e-02,  6.8703e-03,  ...,  1.1002e-02,\n",
            "           -1.1253e-02, -9.8648e-03],\n",
            "          [ 2.2411e-03, -1.5686e-02,  3.5126e-02,  ..., -4.9324e-03,\n",
            "           -3.7231e-02, -2.6505e-02],\n",
            "          [ 1.0132e-02, -2.3972e-02,  5.1697e-02,  ..., -1.1131e-02,\n",
            "           -4.2000e-03,  3.9368e-02]],\n",
            "\n",
            "         [[ 1.0323e-02,  2.2186e-02,  1.1730e-03,  ...,  5.4436e-03,\n",
            "           -1.2558e-02,  1.7303e-02],\n",
            "          [-2.5055e-02, -1.1391e-02, -1.8631e-02,  ...,  4.7417e-03,\n",
            "            4.3440e-04,  1.5656e-02],\n",
            "          [-3.3234e-02, -1.7807e-02, -1.1635e-03,  ..., -2.8015e-02,\n",
            "           -2.9968e-02, -6.5422e-03],\n",
            "          ...,\n",
            "          [ 4.8828e-03, -1.0849e-02,  9.8877e-03,  ...,  1.3313e-02,\n",
            "           -8.7967e-03, -8.7662e-03],\n",
            "          [ 2.7046e-03, -1.3275e-02,  4.0527e-02,  ...,  4.4370e-04,\n",
            "           -3.0518e-02, -2.4918e-02],\n",
            "          [ 8.8272e-03, -2.1423e-02,  5.2643e-02,  ..., -6.3934e-03,\n",
            "           -7.3242e-04,  3.8818e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.9427e-03,  3.4122e-03, -2.5864e-03,  ...,  1.0246e-02,\n",
            "            1.4229e-02,  1.8768e-02],\n",
            "          [-2.8629e-03, -3.3722e-03, -1.3168e-02,  ...,  9.1696e-04,\n",
            "            5.4283e-03,  6.6032e-03],\n",
            "          [-3.9787e-03,  5.2147e-03, -3.4542e-03,  ...,  8.4639e-04,\n",
            "            2.5959e-03,  6.8703e-03],\n",
            "          ...,\n",
            "          [ 5.0049e-03,  4.1161e-03,  2.0683e-04,  ...,  1.6174e-02,\n",
            "            1.4397e-02,  1.2421e-02],\n",
            "          [ 2.5986e-02,  1.5144e-02,  5.7869e-03,  ...,  2.3376e-02,\n",
            "            2.2766e-02,  2.2659e-02],\n",
            "          [ 3.1830e-02,  2.6642e-02,  1.4381e-02,  ...,  2.5604e-02,\n",
            "            2.2263e-02,  2.5238e-02]],\n",
            "\n",
            "         [[ 6.8245e-03,  4.8637e-03, -2.8706e-03,  ...,  1.7715e-02,\n",
            "            1.9653e-02,  2.8290e-02],\n",
            "          [-2.6302e-03, -1.3628e-03, -1.1459e-02,  ...,  5.4474e-03,\n",
            "            6.0501e-03,  1.2810e-02],\n",
            "          [-9.0714e-03,  4.1199e-03, -5.6505e-05,  ...,  9.6178e-04,\n",
            "           -5.9271e-04,  8.8959e-03],\n",
            "          ...,\n",
            "          [ 6.5079e-03,  5.4359e-03,  2.5978e-03,  ...,  2.5131e-02,\n",
            "            2.4902e-02,  2.6871e-02],\n",
            "          [ 3.1647e-02,  2.0325e-02,  8.9188e-03,  ...,  3.4180e-02,\n",
            "            3.3569e-02,  3.9612e-02],\n",
            "          [ 3.8391e-02,  3.2745e-02,  1.8616e-02,  ...,  4.2664e-02,\n",
            "            3.9276e-02,  4.6051e-02]],\n",
            "\n",
            "         [[ 1.6922e-02,  1.5205e-02,  1.1314e-02,  ...,  2.0981e-02,\n",
            "            2.1042e-02,  2.9053e-02],\n",
            "          [ 7.5378e-03,  7.6141e-03,  7.9107e-04,  ...,  7.1449e-03,\n",
            "            7.7019e-03,  1.2169e-02],\n",
            "          [-2.0492e-04,  1.0826e-02,  6.2065e-03,  ...,  3.9253e-03,\n",
            "            3.3913e-03,  9.0408e-03],\n",
            "          ...,\n",
            "          [ 3.7727e-03,  4.7541e-04,  1.6661e-03,  ...,  8.1329e-03,\n",
            "            8.0795e-03,  1.4221e-02],\n",
            "          [ 1.7258e-02,  3.2291e-03,  1.0824e-03,  ...,  1.2383e-02,\n",
            "            1.1887e-02,  1.8204e-02],\n",
            "          [ 2.0462e-02,  1.1421e-02,  3.3875e-03,  ...,  2.1469e-02,\n",
            "            1.5701e-02,  2.2247e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.4652e-04, -3.0947e-04, -4.7588e-04,  ..., -3.1114e-05,\n",
            "            3.1686e-04, -3.0446e-04],\n",
            "          [-1.0872e-04, -7.8440e-04, -8.4782e-04,  ..., -4.0114e-05,\n",
            "           -2.2995e-04, -1.6510e-04],\n",
            "          [-1.3084e-03, -6.0129e-04, -9.7513e-04,  ..., -6.2561e-04,\n",
            "           -5.0831e-04,  5.9748e-04],\n",
            "          ...,\n",
            "          [-9.9087e-04, -6.6948e-04, -4.1127e-06,  ..., -4.7803e-05,\n",
            "           -9.3746e-04, -6.7472e-04],\n",
            "          [-1.0452e-03,  2.7657e-05, -1.3571e-03,  ..., -6.7282e-04,\n",
            "           -1.4048e-03, -1.3056e-03],\n",
            "          [-3.6526e-04, -4.2915e-04, -1.7321e-04,  ..., -1.3387e-04,\n",
            "            3.2640e-04,  1.7679e-04]],\n",
            "\n",
            "         [[ 4.7207e-04, -1.0139e-04,  2.9981e-05,  ...,  2.3270e-04,\n",
            "            1.0500e-03, -5.0831e-04],\n",
            "          [ 9.1600e-04, -1.1677e-04,  5.0116e-04,  ...,  4.8614e-04,\n",
            "            1.0071e-03, -4.9925e-04],\n",
            "          [-5.6362e-04,  2.9826e-04,  1.7405e-03,  ...,  7.9107e-04,\n",
            "            8.0919e-04,  9.7132e-04],\n",
            "          ...,\n",
            "          [ 4.4680e-04,  3.2568e-04,  3.8528e-04,  ...,  1.0176e-03,\n",
            "            6.2895e-04,  4.0889e-04],\n",
            "          [ 8.1253e-04,  4.8256e-04, -9.0313e-04,  ...,  7.1192e-04,\n",
            "           -2.8610e-04,  1.9038e-04],\n",
            "          [ 3.9649e-04, -3.9983e-04,  6.3944e-04,  ..., -6.4135e-05,\n",
            "            1.4138e-04,  4.2319e-04]],\n",
            "\n",
            "         [[ 4.6372e-04,  3.6657e-05, -4.9829e-04,  ..., -2.3520e-04,\n",
            "           -4.3130e-04, -5.6791e-04],\n",
            "          [-2.8670e-05,  2.8181e-04,  3.0351e-04,  ..., -3.4332e-04,\n",
            "            1.9002e-04,  1.8311e-04],\n",
            "          [-1.2398e-03, -4.5681e-04, -4.3273e-04,  ...,  3.8576e-04,\n",
            "            2.3186e-05, -5.7578e-05],\n",
            "          ...,\n",
            "          [ 2.3723e-04, -2.5702e-04,  5.3215e-04,  ...,  4.0674e-04,\n",
            "            1.3471e-04,  6.5708e-04],\n",
            "          [ 6.1452e-05,  3.3474e-04, -6.3944e-04,  ...,  3.9101e-04,\n",
            "            1.0538e-04, -4.9019e-04],\n",
            "          [-1.4639e-04, -4.9543e-04,  1.3185e-04,  ...,  8.1062e-06,\n",
            "           -4.1580e-04,  5.5599e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2093e-02,  1.8646e-02,  5.2299e-03,  ...,  1.1078e-02,\n",
            "            6.1874e-03,  2.6535e-02],\n",
            "          [ 1.1375e-02,  2.9419e-02,  1.3588e-02,  ..., -9.1476e-03,\n",
            "           -1.7639e-02, -3.2768e-03],\n",
            "          [ 2.0905e-02,  3.0792e-02,  1.7181e-02,  ..., -1.1398e-02,\n",
            "           -2.7237e-02, -5.5046e-03],\n",
            "          ...,\n",
            "          [ 3.7140e-02,  2.5406e-02,  6.6452e-03,  ...,  1.8311e-02,\n",
            "            2.8503e-02,  3.6499e-02],\n",
            "          [ 1.9501e-02, -5.5923e-03, -2.1042e-02,  ..., -1.6556e-02,\n",
            "           -2.3010e-02,  5.0926e-03],\n",
            "          [ 1.3893e-02, -2.6947e-02, -5.0720e-02,  ..., -1.2222e-02,\n",
            "           -3.9276e-02,  1.3908e-02]],\n",
            "\n",
            "         [[ 1.4706e-03,  9.1858e-03, -2.5845e-03,  ...,  7.8201e-03,\n",
            "            2.8896e-03,  2.2415e-02],\n",
            "          [ 1.3762e-03,  2.3300e-02,  1.1292e-02,  ..., -8.3771e-03,\n",
            "           -1.8616e-02, -6.5308e-03],\n",
            "          [ 1.0567e-02,  2.6245e-02,  1.8402e-02,  ..., -1.2131e-02,\n",
            "           -3.0472e-02, -9.3613e-03],\n",
            "          ...,\n",
            "          [ 3.4790e-02,  2.1149e-02,  8.6117e-04,  ...,  1.6159e-02,\n",
            "            2.6001e-02,  3.4241e-02],\n",
            "          [ 1.4679e-02, -1.1284e-02, -2.6978e-02,  ..., -1.9974e-02,\n",
            "           -3.0487e-02, -9.5701e-04],\n",
            "          [ 5.8899e-03, -3.8544e-02, -6.2622e-02,  ..., -2.1103e-02,\n",
            "           -5.1422e-02,  4.2267e-03]],\n",
            "\n",
            "         [[-1.0635e-02, -1.9264e-03, -8.6594e-03,  ...,  4.3297e-03,\n",
            "           -2.1572e-03,  1.6891e-02],\n",
            "          [-1.2665e-02,  1.1078e-02,  5.2528e-03,  ..., -8.9035e-03,\n",
            "           -2.1347e-02, -9.2316e-03],\n",
            "          [-3.2139e-03,  1.4435e-02,  1.3611e-02,  ..., -1.2436e-02,\n",
            "           -2.9694e-02, -1.2733e-02],\n",
            "          ...,\n",
            "          [ 3.2898e-02,  2.2339e-02,  7.0572e-03,  ...,  1.0818e-02,\n",
            "            1.9257e-02,  2.4582e-02],\n",
            "          [ 1.3977e-02, -6.0883e-03, -1.4099e-02,  ..., -2.1851e-02,\n",
            "           -3.1281e-02, -9.1248e-03],\n",
            "          [ 6.0272e-03, -3.1830e-02, -5.0995e-02,  ..., -2.3849e-02,\n",
            "           -5.2246e-02, -2.8057e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2202e-02, -7.5188e-03, -2.8854e-02,  ..., -2.2507e-02,\n",
            "            8.1863e-03, -4.8248e-02],\n",
            "          [ 1.6983e-02, -3.1281e-02, -4.2389e-02,  ..., -6.0692e-03,\n",
            "            9.3689e-03, -3.8544e-02],\n",
            "          [ 2.1973e-02, -1.5793e-02, -4.1107e-02,  ...,  4.5807e-02,\n",
            "            2.3956e-02, -1.1154e-02],\n",
            "          ...,\n",
            "          [ 2.8351e-02,  3.7689e-02,  4.1321e-02,  ...,  2.4643e-02,\n",
            "           -1.8654e-03, -1.9440e-02],\n",
            "          [-1.5114e-02, -2.0706e-02, -2.7962e-03,  ...,  2.0645e-02,\n",
            "            8.4381e-03,  1.4282e-02],\n",
            "          [-2.9125e-03, -8.9722e-03,  7.0572e-03,  ...,  1.6861e-02,\n",
            "            1.6689e-03,  1.6891e-02]],\n",
            "\n",
            "         [[ 2.2232e-02, -8.1863e-03, -3.0563e-02,  ..., -1.9592e-02,\n",
            "            1.5839e-02, -4.3243e-02],\n",
            "          [ 1.6281e-02, -3.2288e-02, -4.2389e-02,  ..., -3.5310e-04,\n",
            "            1.8433e-02, -3.1799e-02],\n",
            "          [ 2.0615e-02, -1.6006e-02, -3.9795e-02,  ...,  5.0415e-02,\n",
            "            2.9694e-02, -5.4817e-03],\n",
            "          ...,\n",
            "          [ 4.2480e-02,  5.0110e-02,  4.9622e-02,  ...,  2.2232e-02,\n",
            "           -1.5268e-03, -1.4626e-02],\n",
            "          [-4.3030e-03, -1.0712e-02,  6.6299e-03,  ...,  1.9241e-02,\n",
            "            8.5602e-03,  2.0035e-02],\n",
            "          [ 5.2910e-03, -1.5764e-03,  1.4839e-02,  ...,  1.1139e-02,\n",
            "           -2.3785e-03,  1.9150e-02]],\n",
            "\n",
            "         [[ 6.6986e-03, -1.8478e-02, -3.5858e-02,  ..., -1.4862e-02,\n",
            "            2.1317e-02, -3.0197e-02],\n",
            "          [-5.2166e-04, -4.1260e-02, -4.5319e-02,  ...,  5.1880e-03,\n",
            "            2.4658e-02, -1.7670e-02],\n",
            "          [ 2.0237e-03, -2.8397e-02, -4.5319e-02,  ...,  5.0110e-02,\n",
            "            3.4302e-02,  4.5395e-03],\n",
            "          ...,\n",
            "          [ 3.7628e-02,  4.5807e-02,  4.9103e-02,  ...,  2.0233e-02,\n",
            "            4.1656e-03, -5.9814e-03],\n",
            "          [-5.2338e-03, -9.2239e-03,  1.2566e-02,  ...,  2.1179e-02,\n",
            "            1.6068e-02,  2.5879e-02],\n",
            "          [ 2.6760e-03, -5.6696e-04,  1.7899e-02,  ...,  1.1452e-02,\n",
            "            2.2964e-03,  2.2339e-02]]]], device='cuda:0')), ('base_model.model.model.vision_tower.vision_model.embeddings.position_embedding.weight', Parameter containing:\n",
            "tensor([[ 0.0017,  0.0484, -0.0145,  ...,  0.0005, -0.0566, -0.0468],\n",
            "        [ 0.0112, -0.0406,  0.0338,  ...,  0.0249, -0.0308, -0.0376],\n",
            "        [ 0.0016, -0.0357,  0.0118,  ...,  0.0215, -0.0292, -0.0399],\n",
            "        ...,\n",
            "        [-0.0033, -0.0346, -0.0031,  ..., -0.0287, -0.0336, -0.0360],\n",
            "        [-0.0055, -0.0331, -0.0074,  ..., -0.0286, -0.0326, -0.0354],\n",
            "        [-0.0067, -0.0280, -0.0148,  ..., -0.0182, -0.0268, -0.0353]],\n",
            "       device='cuda:0')), ('base_model.model.model.vision_tower.vision_model.pre_layrnorm.weight', Parameter containing:\n",
            "tensor([0.3306, 0.0026, 0.1605,  ..., 2.1934, 0.0042, 0.0044], device='cuda:0')), ('base_model.model.model.vision_tower.vision_model.pre_layrnorm.bias', Parameter containing:\n",
            "tensor([-0.0044, -0.0450, -0.0473,  ...,  0.0395, -0.1389, -0.0134],\n",
            "       device='cuda:0'))]\n",
            "Loading dataset from meld_with_rationales.jsonl\n",
            "Dataset size: 100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1811879401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1811879401.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# 6) SFTConfig + trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     training_args = SFTConfig(\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SFTConfig.__init__() got an unexpected keyword argument 'max_seq_length'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "finetune_after_script_b.py\n",
        "\n",
        "Uses robust_llava_loader.load_model_and_tokenizer() (script B output) to load the model,\n",
        "auto-detects good LoRA target_modules, and runs PEFT (LoRA) fine-tuning with trl.SFTTrainer.\n",
        "\n",
        "Usage: python finetune_after_script_b.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "\n",
        "# try to import helper for preparing k-bit training (peft versions vary)\n",
        "try:\n",
        "    from peft import prepare_model_for_kbit_training\n",
        "except Exception:\n",
        "    try:\n",
        "        from peft.utils import prepare_model_for_kbit_training\n",
        "    except Exception:\n",
        "        prepare_model_for_kbit_training = None\n",
        "\n",
        "# Add the src directory to the Python path so we can import robust_llava_loader\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
        "from robust_llava_loader import load_model_and_tokenizer\n",
        "\n",
        "# --------------------- USER CONFIG ---------------------\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "DATASET_PATH = \"meld_with_rationales.jsonl\"   # jsonl containing utterance, sentiment, rationale\n",
        "OUTPUT_DIR = \"./llava-peft-adapters-auto\"\n",
        "USE_4BIT_IF_AVAILABLE = True\n",
        "MAX_SEQ_LENGTH = 512\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "SAVE_STEPS = 200\n",
        "LOGGING_STEPS = 20\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# helper: create the training prompt\n",
        "def build_prompt(example):\n",
        "    return (\n",
        "        \"You are a sentiment analysis expert. Analyze the following utterance and provide \"\n",
        "        \"the sentiment along with a step-by-step rationale for your decision.\\n\\n\"\n",
        "        \"### Utterance:\\n\"\n",
        "        f\"{example.get('utterance','')}\\n\\n\"\n",
        "        \"### Analysis:\\n\"\n",
        "        f\"Sentiment: {example.get('sentiment','')}\\n\"\n",
        "        f\"Rationale: {example.get('rationale','')}\"\n",
        "    )\n",
        "\n",
        "# helper: scan model.named_modules() and choose candidate target module name substrings\n",
        "def auto_detect_target_module_names(model, prefer_text=True):\n",
        "    \"\"\"\n",
        "    Returns a list of module-name substrings to use in LoraConfig.target_modules.\n",
        "    Strategy:\n",
        "      - Collect names of submodules that look like projections (q_proj, k_proj, v_proj, out_proj, o_proj)\n",
        "      - Prefer modules under 'model' that contain tokens like 'self_attn', 'attn', 'q_proj' etc.\n",
        "      - If prefer_text=True, try to exclude modules under vision tower (module name containing 'vision' or 'vision_tower')\n",
        "    \"\"\"\n",
        "    proj_patterns = set()\n",
        "    name_list = [n for n, _ in model.named_modules()]\n",
        "\n",
        "    for n in name_list:\n",
        "        # skip top-level empty name\n",
        "        if not n:\n",
        "            continue\n",
        "        # skip vision modules if preferring text modules\n",
        "        if prefer_text and (\"vision\" in n or \"vision_tower\" in n or \"vision_model\" in n):\n",
        "            continue\n",
        "        # find typical projection/fc names in module path\n",
        "        if re.search(r\"(q_proj|k_proj|v_proj|o_proj|out_proj|gate_proj|up_proj|down_proj|fc1|fc2)\", n):\n",
        "            # extract final token (last part after '.')\n",
        "            final = n.split(\".\")[-1]\n",
        "            proj_patterns.add(final)\n",
        "    # fallback if empty\n",
        "    if not proj_patterns:\n",
        "        # default common names\n",
        "        proj_patterns = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    # keep consistent ordering and return as list\n",
        "    return sorted(list(proj_patterns))\n",
        "\n",
        "def format_and_map(example, tokenizer):\n",
        "    text = build_prompt(example)\n",
        "    eos = tokenizer.eos_token or \"\"\n",
        "    return {\"text\": text + eos}\n",
        "\n",
        "def main():\n",
        "    # decide 4-bit usage\n",
        "    use_4bit = False\n",
        "    if USE_4BIT_IF_AVAILABLE:\n",
        "        try:\n",
        "            import bitsandbytes  # noqa: F401\n",
        "            use_4bit = True\n",
        "        except Exception:\n",
        "            print(\"bitsandbytes not installed/found — running without 4-bit.\")\n",
        "\n",
        "    # 1) Load model + tokenizer (robust loader)\n",
        "    print(\"Loading model + tokenizer (robust loader)...\")\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name=BASE_MODEL_NAME, use_4bit=use_4bit)\n",
        "    print(\"Loaded model and tokenizer. Model dtype hint:\", getattr(model, \"dtype\", None))\n",
        "    model.config.use_cache = False\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Auto-detect target_modules for LoRA (based on model module names)\n",
        "    print(\"Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\")\n",
        "    detected = auto_detect_target_module_names(model, prefer_text=True)\n",
        "    print(\"Detected target-module name tokens (candidates):\", detected)\n",
        "\n",
        "    # We'll use these tokens as LoraConfig.target_modules (PEFT expects substrings)\n",
        "    target_modules = detected\n",
        "\n",
        "    # 3) Prepare model for k-bit training (if using 4-bit & helper present)\n",
        "    if use_4bit:\n",
        "        if prepare_model_for_kbit_training is not None:\n",
        "            print(\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\")\n",
        "            model = prepare_model_for_kbit_training(model)\n",
        "        else:\n",
        "            print(\"prepare_model_for_kbit_training not available in this peft version — continuing.\")\n",
        "\n",
        "    # 4) Create LoraConfig and wrap model\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    print(\"Applying LoRA with LoraConfig:\", lora_cfg)\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    print(\"PEFT/LoRA applied. Peft model keys:\", list(model.named_parameters())[:5])\n",
        "\n",
        "    # 5) Load and format dataset\n",
        "    print(\"Loading dataset from\", DATASET_PATH)\n",
        "    ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(\"Dataset size:\", len(ds))\n",
        "    # map to `text` field expected by SFTTrainer\n",
        "    ds = ds.map(lambda ex: format_and_map(ex, tokenizer), remove_columns=ds.column_names)\n",
        "\n",
        "    # import here to avoid top-level dependency until needed\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "    # 6) SFTConfig + trainer\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=use_4bit or (torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 12 * 1024 ** 2),\n",
        "        save_steps=SAVE_STEPS,\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        packing=False,\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=ds,\n",
        "        peft_config=lora_cfg,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args, # Pass the SFTConfig object here\n",
        "    )\n",
        "\n",
        "    # 7) Dry-run: single step to validate forward/backward\n",
        "    print(\"Running a 1-step dry-run to validate training loop...\")\n",
        "    try:\n",
        "        trainer.train(max_steps=1)\n",
        "        print(\"Dry-run succeeded.\")\n",
        "    except Exception as e:\n",
        "        print(\"Dry-run failed — inspect traceback. Error:\", e)\n",
        "        raise\n",
        "\n",
        "    # 8) Full training\n",
        "    print(\"Starting full training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # 9) Save PEFT adapters\n",
        "    print(\"Saving adapters to:\", OUTPUT_DIR)\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    print(\"Saved. You can load later with PeftModel.from_pretrained(base_model, OUTPUT_DIR)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YAFcBo8uVALa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "2037afa7f4034f3282f071fc8838c720",
            "d09204c09d1342ccb0276f258fe356af",
            "ebd63ff154784158bec870b95f2bf188",
            "6e38cfaf7db7474c86891719d85a22e0",
            "69a9df54bfc14094af502b5b36e478c5",
            "e8ed2b8a11c64fbda8bc48085967dd2d",
            "291365ca472e44da898bb6048063d962",
            "15aba306c025462bbefbcaffcc703a10",
            "27db83c4eaca42708a5a44b02f6588ff",
            "cf95db53da5444a587a4871459477970",
            "969c7195ff504b76a5eeeece0fd0b981"
          ]
        },
        "outputId": "9ef2fd21-1ed3-4861-a3ea-fd72d1053a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model + tokenizer (robust loader)...\n",
            "Trying to load LlavaForConditionalGeneration with device_map=auto (quant_config=yes)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2037afa7f4034f3282f071fc8838c720"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model and tokenizer. Model dtype hint: torch.float16\n",
            "Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\n",
            "Detected target-module name tokens (candidates): ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
            "Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 224.12 MiB is free. Process 17681 has 14.52 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 563.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-509398528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-509398528.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prepare_model_for_kbit_training not available in this peft version — continuing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             ) and param.__class__.__name__ != \"Params4bit\":\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     if (\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 224.12 MiB is free. Process 17681 has 14.52 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 563.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "finetune_after_script_b.py\n",
        "\n",
        "Uses robust_llava_loader.load_model_and_tokenizer() (script B output) to load the model,\n",
        "auto-detects good LoRA target_modules, and runs PEFT (LoRA) fine-tuning with trl.SFTTrainer.\n",
        "\n",
        "Usage: python finetune_after_script_b.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "\n",
        "# try to import helper for preparing k-bit training (peft versions vary)\n",
        "try:\n",
        "    from peft import prepare_model_for_kbit_training\n",
        "except Exception:\n",
        "    try:\n",
        "        from peft.utils import prepare_model_for_kbit_training\n",
        "    except Exception:\n",
        "        prepare_model_for_kbit_training = None\n",
        "\n",
        "# Add the src directory to the Python path so we can import robust_llava_loader\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
        "from robust_llava_loader import load_model_and_tokenizer\n",
        "\n",
        "# --------------------- USER CONFIG ---------------------\n",
        "BASE_MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
        "DATASET_PATH = \"meld_with_rationales.jsonl\"   # jsonl containing utterance, sentiment, rationale\n",
        "OUTPUT_DIR = \"./llava-peft-adapters-auto\"\n",
        "USE_4BIT_IF_AVAILABLE = True\n",
        "MAX_SEQ_LENGTH = 512\n",
        "PER_DEVICE_BATCH_SIZE = 1  # Reduced batch size further\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4  # Increased accumulation steps further\n",
        "SAVE_STEPS = 200\n",
        "LOGGING_STEPS = 20\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# helper: create the training prompt\n",
        "def build_prompt(example):\n",
        "    return (\n",
        "        \"You are a sentiment analysis expert. Analyze the following utterance and provide \"\n",
        "        \"the sentiment along with a step-by-step rationale for your decision.\\n\\n\"\n",
        "        \"### Utterance:\\n\"\n",
        "        f\"{example.get('utterance','')}\\n\\n\"\n",
        "        \"### Analysis:\\n\"\n",
        "        f\"Sentiment: {example.get('sentiment','')}\\n\"\n",
        "        f\"Rationale: {example.get('rationale','')}\"\n",
        "    )\n",
        "\n",
        "# helper: scan model.named_modules() and choose candidate target module name substrings\n",
        "def auto_detect_target_module_names(model, prefer_text=True):\n",
        "    \"\"\"\n",
        "    Returns a list of module-name substrings to use in LoraConfig.target_modules.\n",
        "    Strategy:\n",
        "      - Collect names of submodules that look like projections (q_proj, k_proj, v_proj, out_proj, o_proj)\n",
        "      - Prefer modules under 'model' that contain tokens like 'self_attn', 'attn', 'q_proj' etc.\n",
        "      - If prefer_text=True, try to exclude modules under vision tower (module name containing 'vision' or 'vision_tower')\n",
        "    \"\"\"\n",
        "    proj_patterns = set()\n",
        "    name_list = [n for n, _ in model.named_modules()]\n",
        "\n",
        "    for n in name_list:\n",
        "        # skip top-level empty name\n",
        "        if not n:\n",
        "            continue\n",
        "        # skip vision modules if preferring text modules\n",
        "        if prefer_text and (\"vision\" in n or \"vision_tower\" in n or \"vision_model\" in n):\n",
        "            continue\n",
        "        # find typical projection/fc names in module path\n",
        "        if re.search(r\"(q_proj|k_proj|v_proj|o_proj|out_proj|gate_proj|up_proj|down_proj|fc1|fc2)\", n):\n",
        "            # extract final token (last part after '.')\n",
        "            final = n.split(\".\")[-1]\n",
        "            proj_patterns.add(final)\n",
        "    # fallback if empty\n",
        "    if not proj_patterns:\n",
        "        # default common names\n",
        "        proj_patterns = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    # keep consistent ordering and return as list\n",
        "    return sorted(list(proj_patterns))\n",
        "\n",
        "# UPDATED: tokenize here and enforce max_length/truncation/padding\n",
        "def format_and_map(example, tokenizer):\n",
        "    text = build_prompt(example)\n",
        "    eos = tokenizer.eos_token or \"\"\n",
        "    full = text + eos\n",
        "    # ensure the tokenizer has a pad token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "    encoded = tokenizer(\n",
        "        full,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    # return input_ids & attention_mask — SFTTrainer can be told to use 'input_ids' as dataset_text_field\n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"],\n",
        "        \"attention_mask\": encoded[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # decide 4-bit usage\n",
        "    use_4bit = False\n",
        "    if USE_4BIT_IF_AVAILABLE:\n",
        "        try:\n",
        "            import bitsandbytes  # noqa: F401\n",
        "            use_4bit = True\n",
        "        except Exception:\n",
        "            print(\"bitsandbytes not installed/found — running without 4-bit.\")\n",
        "\n",
        "    # 1) Load model + tokenizer (robust loader)\n",
        "    print(\"Loading model + tokenizer (robust loader)...\")\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name=BASE_MODEL_NAME, use_4bit=use_4bit)\n",
        "    print(\"Loaded model and tokenizer. Model dtype hint:\", getattr(model, \"dtype\", None))\n",
        "    model.config.use_cache = False\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Auto-detect target_modules for LoRA (based on model module names)\n",
        "    print(\"Auto-detecting candidate LoRA target module name tokens from model.named_modules()...\")\n",
        "    detected = auto_detect_target_module_names(model, prefer_text=True)\n",
        "    print(\"Detected target-module name tokens (candidates):\", detected)\n",
        "\n",
        "    # We'll use these tokens as LoraConfig.target_modules (PEFT expects substrings)\n",
        "    target_modules = detected\n",
        "\n",
        "    # 3) Prepare model for k-bit training (if using 4-bit & helper present)\n",
        "    if use_4bit:\n",
        "        if prepare_model_for_kbit_training is not None:\n",
        "            print(\"Preparing model for k-bit training (peft.prepare_model_for_kbit_training)...\")\n",
        "            model = prepare_model_for_kbit_training(model)\n",
        "        else:\n",
        "            print(\"prepare_model_for_kbit_training not available in this peft version — continuing.\")\n",
        "\n",
        "    # 4) Create LoraConfig and wrap model\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    print(\"Applying LoRA with LoraConfig:\", lora_cfg)\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    print(\"PEFT/LoRA applied. Peft model keys:\", list(model.named_parameters())[:5])\n",
        "\n",
        "    # 5) Load and format dataset\n",
        "    print(\"Loading dataset from\", DATASET_PATH)\n",
        "    ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "    print(\"Dataset size:\", len(ds))\n",
        "    # tokenize & create input_ids & attention_mask fields (and remove other columns)\n",
        "    ds = ds.map(lambda ex: format_and_map(ex, tokenizer), remove_columns=ds.column_names)\n",
        "\n",
        "    # Explicitly remove the 'images' column if it exists (it shouldn't in this dataset, but trainer expects it)\n",
        "    if 'images' in ds.column_names:\n",
        "        print(\"Removing 'images' column from dataset...\")\n",
        "        ds = ds.remove_columns(\"images\")\n",
        "\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "    # 6) SFTConfig + trainer\n",
        "    # NOTE: removed max_seq_length from SFTConfig (it caused your TypeError)\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=use_4bit or (torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 12 * 1024 ** 2),\n",
        "        save_steps=SAVE_STEPS,\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "        dataset_text_field=\"input_ids\",   # <-- point trainer to tokenized input_ids field\n",
        "        packing=False,\n",
        "        remove_unused_columns=False, # <-- Add this line\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=ds,\n",
        "        peft_config=lora_cfg,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "       # 7) Dry-run: single step to validate forward/backward\n",
        "    print(\"Running a 1-step dry-run to validate training loop...\")\n",
        "    try:\n",
        "        # Set max_steps in the training_args for the dry run\n",
        "        trainer.args.max_steps = 1\n",
        "        trainer.train() # Call train() without the invalid argument\n",
        "        print(\"Dry-run succeeded.\")\n",
        "    except Exception as e:\n",
        "        print(\"Dry-run failed — inspect traceback. Error:\", e)\n",
        "        # It's good practice to reset the args even if it fails\n",
        "        trainer.args.max_steps = -1\n",
        "        raise\n",
        "\n",
        "    print(\"Saving adapters to:\", OUTPUT_DIR)\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    print(\"Saved. You can load later with PeftModel.from_pretrained(base_model, OUTPUT_DIR)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkKd6kxXXHG6"
      },
      "outputs": [],
      "source": [
        "# (Empty cell placeholder removed for clarity)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5613a6c3ccd4077ba3b453d07fdda93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a9e5dcada3f439d85db8f5fcb013994",
              "IPY_MODEL_8b63505faecd4cb3bf6740dd7c3d7cfb",
              "IPY_MODEL_2f34c26d434546e5a9788e22b6556187"
            ],
            "layout": "IPY_MODEL_6b7986ae63c940f98d85183bf51d1835",
            "tabbable": null,
            "tooltip": null
          }
        },
        "0a9e5dcada3f439d85db8f5fcb013994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9ad458f171e34f0ea30b1f7940dd3728",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d98cb4daa94d81aed7d5c6107e39ea",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8b63505faecd4cb3bf6740dd7c3d7cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e29c7397e341446b84390c1631351c1a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84e099d27c5c49f19a81a23d1db502aa",
            "tabbable": null,
            "tooltip": null,
            "value": 3
          }
        },
        "2f34c26d434546e5a9788e22b6556187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_75675b02d22f40fbbb27c2cc212522b6",
            "placeholder": "​",
            "style": "IPY_MODEL_69359fafe2b44b85978372ffcd8e4795",
            "tabbable": null,
            "tooltip": null,
            "value": " 3/3 [01:11&lt;00:00, 23.25s/it]"
          }
        },
        "6b7986ae63c940f98d85183bf51d1835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad458f171e34f0ea30b1f7940dd3728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d98cb4daa94d81aed7d5c6107e39ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "e29c7397e341446b84390c1631351c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e099d27c5c49f19a81a23d1db502aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75675b02d22f40fbbb27c2cc212522b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69359fafe2b44b85978372ffcd8e4795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "2037afa7f4034f3282f071fc8838c720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d09204c09d1342ccb0276f258fe356af",
              "IPY_MODEL_ebd63ff154784158bec870b95f2bf188",
              "IPY_MODEL_6e38cfaf7db7474c86891719d85a22e0"
            ],
            "layout": "IPY_MODEL_69a9df54bfc14094af502b5b36e478c5",
            "tabbable": null,
            "tooltip": null
          }
        },
        "d09204c09d1342ccb0276f258fe356af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e8ed2b8a11c64fbda8bc48085967dd2d",
            "placeholder": "​",
            "style": "IPY_MODEL_291365ca472e44da898bb6048063d962",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ebd63ff154784158bec870b95f2bf188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_15aba306c025462bbefbcaffcc703a10",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27db83c4eaca42708a5a44b02f6588ff",
            "tabbable": null,
            "tooltip": null,
            "value": 3
          }
        },
        "6e38cfaf7db7474c86891719d85a22e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_cf95db53da5444a587a4871459477970",
            "placeholder": "​",
            "style": "IPY_MODEL_969c7195ff504b76a5eeeece0fd0b981",
            "tabbable": null,
            "tooltip": null,
            "value": " 3/3 [01:04&lt;00:00, 20.86s/it]"
          }
        },
        "69a9df54bfc14094af502b5b36e478c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ed2b8a11c64fbda8bc48085967dd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291365ca472e44da898bb6048063d962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "15aba306c025462bbefbcaffcc703a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27db83c4eaca42708a5a44b02f6588ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf95db53da5444a587a4871459477970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969c7195ff504b76a5eeeece0fd0b981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}