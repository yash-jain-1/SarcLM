# -*- coding: utf-8 -*-
"""MELD Data Preparation with CoT Generation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1evnqSuDOuP3G7Ci7bC9ie8croVDcmQAy
"""

from google.colab import userdata

import json
import os
import time
import random
import pandas as pd
import requests

# --- Configuration ---
# API_KEY = os.environ.get("GOOGLE_API_KEY", "")  # prefer env var
API_KEY = userdata.get('GOOGLE_API_KEY')
MODEL_NAME = "gemini-2.5-flash-preview-05-20"
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}"

# Input and Output file paths
INPUT_CSV_PATH = '/content/drive/MyDrive/Capstone/train.csv'
OUTPUT_JSONL_PATH = 'meld_with_rationales.jsonl'
SAMPLE_SIZE = 100 # Set to a small number for testing, or len(df) to process the whole file.

COT_PROMPT_TEMPLATE = """
Act as a Multimodal Sentiment Analysis Expert. Your task is to generate a step-by-step reasoning (a Chain-of-Thought rationale) for why the given text has a specific sentiment. The rationale should be clear, logical, and break down the analysis into distinct steps.

Here is an example:
Text: "Oh, that's just great. Another flat tire."
Sentiment: "negative"
Rationale:
1.  **Identify the core event:** The speaker has a "flat tire."
2.  **Analyze the context:** Flat tires are universally considered inconvenient and negative events.
3.  **Analyze the phrasing:** The phrase "Oh, that's just great" is a common example of sarcasm in English. It is used to express the opposite of its literal meaning, indicating frustration or annoyance.
4.  **Synthesize:** The combination of a negative event (the flat tire) with sarcastic phrasing strongly points to a negative sentiment from the speaker.

---

Now, generate a rationale for the following:
Text: "{utterance}"
Sentiment: "{sentiment}"
Rationale:
"""

def find_first_string(obj):
    """
    Recursively search a JSON-like object for the first non-empty string.
    Prioritizes keys commonly used to hold text.
    """
    if obj is None:
        return None
    if isinstance(obj, str):
        s = obj.strip()
        return s if s else None
    if isinstance(obj, dict):
        # check high-probability keys first
        for key in ('text', 'output_text', 'message', 'content', 'response', 'parts', 'output'):
            if key in obj:
                res = find_first_string(obj[key])
                if res:
                    return res
        # fallback to iterating all keys
        for v in obj.values():
            res = find_first_string(v)
            if res:
                return res
    if isinstance(obj, (list, tuple)):
        for item in obj:
            res = find_first_string(item)
            if res:
                return res
    return None

def extract_text_from_result(result):
    """
    Try a few reasonable places the API might return the text in.
    Returns the first discovered string or None.
    """
    if not isinstance(result, dict):
        return find_first_string(result)

    # Try 'candidates' first (commonly used)
    if 'candidates' in result and isinstance(result['candidates'], (list, tuple)):
        for cand in result['candidates']:
            # Candidate could be dict or string
            text = find_first_string(cand)
            if text:
                return text

    # Try other common top-level keys
    for key in ('output', 'outputs', 'response', 'result', 'choices'):
        if key in result:
            text = find_first_string(result[key])
            if text:
                return text

    # As a last resort, search the entire result
    return find_first_string(result)

def generate_rationale(utterance, sentiment, max_retries=5):
    prompt = COT_PROMPT_TEMPLATE.format(utterance=utterance, sentiment=sentiment)

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.4,
            "topP": 1,
            "topK": 32,
            "maxOutputTokens": 256,
        }
    }

    headers = {'Content-Type': 'application/json'}
    timeout_seconds = 30

    for attempt in range(1, max_retries + 1):
        try:
            resp = requests.post(API_URL, json=payload, headers=headers, timeout=timeout_seconds)
            resp.raise_for_status()
            result = resp.json()

            # Robust extraction
            text = extract_text_from_result(result)
            if text:
                return text.strip()
            else:
                # Log full response (truncated) for debugging and return an error tag
                dump = json.dumps(result, indent=2)[:4000]
                print("Warning: Couldn't find text in API response. Full response (truncated):")
                print(dump)
                return "Error: No rationale generated (unexpected response shape)."

        except requests.exceptions.RequestException as e:
            print(f"Request error on attempt {attempt}/{max_retries}: {e}")
            if attempt == max_retries:
                return f"Error: API call failed after {max_retries} attempts: {e}"
            # exponential backoff with jitter
            backoff = (2 ** (attempt - 1)) + random.random()
            print(f"Retrying in {backoff:.1f}s...")
            time.sleep(backoff)

    return "Error: Max retries exceeded."

from google.colab import drive
drive.mount('/content/drive')

def main():
    print("Starting data preparation process...")
    if not os.path.exists(INPUT_CSV_PATH):
        print(f"Input file not found at '{INPUT_CSV_PATH}'. Creating a small dummy file for demonstration.")
        dummy_data = {
            'Utterance': [
                'Oh my God, he’s lost it. He’s totally lost it.',
                'What? No. No, I just wanna hear you say it.',
                "You're right, I'm sorry."
            ],
            'Sentiment': ['negative', 'neutral', 'positive'],
            'Emotion': ['surprise', 'neutral', 'sadness']
        }
        pd.DataFrame(dummy_data).to_csv(INPUT_CSV_PATH, index=False)

    df = pd.read_csv(INPUT_CSV_PATH)

    if SAMPLE_SIZE < len(df):
        print(f"Using a sample of {SAMPLE_SIZE} rows from the dataset.")
        df_sample = df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)
    else:
        print(f"Processing the full dataset of {len(df)} rows.")
        df_sample = df.reset_index(drop=True)

    total_rows = len(df_sample)
    print(f"Total rows to process: {total_rows}")

    with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f_out:
        for idx, row in df_sample.iterrows():
            print(f"Processing row {idx+1}/{total_rows}...")
            utterance = str(row.get('Utterance', '')).strip()
            sentiment = str(row.get('Sentiment', '')).strip()
            emotion = row.get('Emotion', None)

            if not utterance:
                rationale = "Error: empty utterance"
            else:
                rationale = generate_rationale(utterance, sentiment)

            output_record = {
                'utterance': utterance,
                'sentiment': sentiment,
                'emotion': emotion,
                'rationale': rationale
            }

            f_out.write(json.dumps(output_record, ensure_ascii=False) + '\n')
            # small pause to be polite to API / avoid rate limits
            time.sleep(0.25)

    print("\nData preparation complete!")
    print(f"Output saved to '{OUTPUT_JSONL_PATH}'")
    print("A few example records from the output file:")
    with open(OUTPUT_JSONL_PATH, 'r', encoding='utf-8') as f_in:
        for i, line in enumerate(f_in):
            if i >= 3:
                break
            print(line.strip())

if __name__ == "__main__":
    main()

